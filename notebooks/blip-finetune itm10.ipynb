{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP finetuning to target task\n",
    "\n",
    "TODO: rewrite this to reflect the latest changes\n",
    "\n",
    "Sample is formed from a single row of dataset:\n",
    "$$\\operatorname{batch} = ((E_t, E_{i_0}), (E_t, E_{i_1}), ..., (E_t, E_{i_9})); \\operatorname{batch} : R^{10 \\times (E_t + E_i)}$$\n",
    "ITM predicts probas for $y = 0$, $y = 1$\n",
    "$$\\operatorname{ITM} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10 \\times 2}$$\n",
    "Model is defined as:\n",
    "$$\\operatorname{F} = \\operatorname{softmax} \\circ \\operatorname{ITM} \\circ \\operatorname{batch}$$\n",
    "$$\\operatorname{F} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10}$$\n",
    "So, this definition is for a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "import torch.nn as nn\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.utils import evaluate, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "HEAD = \"itm\" # \"itm\" | \"itc\" | \"mean\"\n",
    "MODEL_VERSION = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"checkpoints\").resolve() / f\"BLIP-{HEAD}-{MODEL_VERSION}\"\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:1\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "# WARNING: this is specific to my setup\n",
    "DEVICE = torch.device(\"cuda:1\") # 3060\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 32\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\"\n",
    "NUM_EPOCHS = 20\n",
    "NUM_PICS = 10\n",
    "WARMUP_STEPS_FRAC = 0.1\n",
    "STEPS_BETWEEN_EVAL = 5\n",
    "GRAD_ACCUM_STEPS = 256\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_EVAL\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.001\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "VALIDATION_BATCH_SIZE = 3\n",
    "HEAD_SUM_BIAS_ENABLED = True\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM_STEPS * TRAIN_BATCH_SIZE\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def infinite_repeat(value):\n",
    "    while True:\n",
    "        yield value\n",
    "\n",
    "def concat_iters(*iterables):\n",
    "    for it in iterables:\n",
    "        for value in it:\n",
    "            yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ItmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        images_path: Path,\n",
    "        text_processor,\n",
    "        vis_processor,\n",
    "        use_context_as_text: bool = True,\n",
    "        enable_cache: bool = False,\n",
    "    ) -> None:\n",
    "        self.df = df\n",
    "        self.images_path = images_path\n",
    "        self.text_processor = text_processor\n",
    "        self.vis_processor = vis_processor\n",
    "        self.tokens_cache = dict()\n",
    "        self.image_tensor_cache = dict()\n",
    "        self.enable_cache = enable_cache\n",
    "        self.text_field = \"context\" if use_context_as_text else \"word\"\n",
    "        self.labels_map = self._gen_labels()\n",
    "\n",
    "    def _gen_labels(self) -> Dict[int, int]: # index to label\n",
    "        labels = self.df[\"label\"].values\n",
    "        zips = []\n",
    "        for i in range(NUM_PICS):\n",
    "            images = self.df[f\"image{i}\"].values\n",
    "            zips.append(zip(np.argwhere(labels == images).reshape(-1), infinite_repeat(i)))\n",
    "        return dict(concat_iters(*tuple(zips)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _make_image_tensor(self, name: str) -> torch.Tensor:\n",
    "        return self.vis_processor(Image.open(self.images_path / name).convert(\"RGB\"))\n",
    "\n",
    "    def _get_image_tensor(self, name: str) -> Image:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_image_tensor(name)\n",
    "        if name in self.image_tensor_cache:\n",
    "            return self.image_tensor_cache[name]\n",
    "        t = self._make_image_tensor(name)\n",
    "        self.image_tensor_cache[name] = t\n",
    "        return t\n",
    "\n",
    "    def _get_image_batch(self, idx: int) -> torch.Tensor:\n",
    "        row = self.df.iloc[idx]\n",
    "        return torch.stack([self._get_image_tensor(row[f\"image{i}\"]) for i in range(NUM_PICS)])\n",
    "\n",
    "    def _make_tokens(self, idx: int) -> BatchEncoding:\n",
    "        return self.text_processor(self.df.iloc[idx][self.text_field])\n",
    "    \n",
    "    def _get_tokens(self, idx: int) -> BatchEncoding:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_tokens(idx)\n",
    "        if idx in self.tokens_cache:\n",
    "            return self.tokens_cache[idx]\n",
    "        t = self._make_tokens(idx)\n",
    "        self.tokens_cache[idx] = t\n",
    "        return t\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, BatchEncoding, int]]:\n",
    "        # makes a batch for each row!\n",
    "        return {\n",
    "            \"text\": self._get_tokens(idx),\n",
    "            \"images\": self._get_image_batch(idx),\n",
    "            \"label\": self.labels_map[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = ItmDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val_ds = ItmDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "test_ds = ItmDataset(\n",
    "    df=test_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "train_l = len(train_dl)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_l = len(val_dl)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=1, num_workers=NUM_WORKERS, persistent_workers=True, shuffle=False)\n",
    "test_l = len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def to_device(object, device):\n",
    "    if not isinstance(object, dict):\n",
    "        raise NotImplementedError(\"Implement other types than dict if needed!\")\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in object.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        blip_model: BlipBase,\n",
    "        match_head: str = \"itm\",\n",
    "        head_sum_bias_enabled: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.blip_model = blip_model\n",
    "        self.match_head = match_head\n",
    "        if self.match_head == \"mean\":\n",
    "            self.head_combiner = nn.Linear(2, 1, bias=head_sum_bias_enabled)\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # TODO: move all this batch-dependent stuff to collate_fn?\n",
    "        # TODO: optimize!\n",
    "        # text: str\n",
    "        # image: \n",
    "        images_shape = inputs[\"images\"].shape # image: (B, NUM_PICS, C, H, W)\n",
    "        batch_size = images_shape[0]\n",
    "        text_input = []\n",
    "        for t in inputs[\"text\"]:\n",
    "            for _ in range(NUM_PICS):\n",
    "                text_input.append(t)\n",
    "        images_input = inputs[\"images\"].reshape(batch_size * NUM_PICS, images_shape[2], images_shape[3], images_shape[4]) # image: (B * NUM_PICS, C, H, W)\n",
    "        # (B * X, 2)\n",
    "        if self.match_head == \"itm\":\n",
    "            batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=self.match_head).reshape(batch_size, NUM_PICS, 2) # (B, NUM_PICS, 2)\n",
    "            batch_probas = F.softmax(batch_outputs[:, :, 1], dim=1)\n",
    "        elif self.match_head == \"itc\":\n",
    "            batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=self.match_head).reshape(batch_size, NUM_PICS) # (B * NUM_PICS) -> (B, NUM_PICS)\n",
    "            # hugginface VisionTextEncoder see cos * N before softmax\n",
    "            # TODO: N as hyperparam const\n",
    "            # TODO: or learnable param\n",
    "            batch_probas = F.softmax(batch_outputs, dim=1) # softmax(cosine similarity) => =(\n",
    "        elif self.match_head == \"mean\":\n",
    "            raise NotImplementedError(\"Implement me!\")\n",
    "            # Warning: not tested\n",
    "            # TODO: replace with mean(p_itm, p_itc)\n",
    "            # itm_batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=\"itm\").reshape(batch_size, 10, 2)\n",
    "            # itc_batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=\"itc\").reshape(batch_size, 10)\n",
    "            # dual_probas = torch.stack([\n",
    "            #     torch.stack([F.softmax(itm_batch_outputs[i, :, 1], dim=0) for i in range(batch_size)]).reshape(batch_size * 10),\n",
    "            #     torch.stack([F.softmax(batch_outputs[i, :], dim=0) for i in range(batch_size)]).reshape(batch_size * 10),\n",
    "            # ], dim=1)\n",
    "            # batch_ouputs = F.softmax(self.head_combiner(dual_probas), dim=0).reshape(batch_size, 10)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected value for match_head parameter \\\"{self.match_head}\\\". Allowed values: \\\"itm\\\", \\\"itc\\\" or \\\"mean\\\".\")\n",
    "        return batch_probas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = Classifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "metric2name = {\n",
    "    \"acc1\": \"Accuracy@Top1\",\n",
    "    \"acc3\": \"Accuracy@Top3\",\n",
    "    \"mrr\": \"Mean Reciprocal Rank\",\n",
    "}\n",
    "\n",
    "labels_range = np.arange(NUM_PICS)\n",
    "\n",
    "def eval_batch(labels, preds):\n",
    "    labels = labels.numpy(force=True)\n",
    "    preds = preds.numpy(force=True)\n",
    "    return {\n",
    "        \"acc1\": top_k_accuracy_score(labels, preds, k=1, labels=labels_range), \n",
    "        \"acc3\": top_k_accuracy_score(labels, preds, k=3, labels=labels_range),\n",
    "        \"mrr\": mrr(labels, preds),\n",
    "    }\n",
    "\n",
    "def sum_scores(scores, new_scores):\n",
    "    return {k: scores[k] + new_scores[k] for k in scores}\n",
    "\n",
    "def div_scores(scores, n):\n",
    "    return {k: v / n for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476 training steps which include 47 warmup ones\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "num_training_steps = int(NUM_EPOCHS * (train_l / GRAD_ACCUM_STEPS))\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_STEPS_FRAC)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implement validation of untuned model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/476 [15:22<24:05:02, 184.08s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "INFO:root:[0:5] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-5.pt\"\n",
      "  2%|▏         | 8/476 [38:50<41:16:08, 317.45s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  2%|▏         | 9/476 [41:53<35:44:05, 275.47s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  2%|▏         | 10/476 [44:56<31:58:08, 246.97s/it]INFO:root:[0:10] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-10.pt\"\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  3%|▎         | 15/476 [1:14:30<32:44:40, 255.71s/it]INFO:root:[0:15] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-15.pt\"\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  4%|▍         | 20/476 [1:44:03<32:32:51, 256.96s/it]INFO:root:[0:20] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-20.pt\"\n",
      "  5%|▍         | 22/476 [2:04:26<50:21:58, 399.38s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  5%|▌         | 25/476 [2:13:36<32:15:08, 257.45s/it]INFO:root:[1:25] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-25.pt\"\n",
      "  6%|▋         | 30/476 [2:43:05<31:49:47, 256.92s/it]INFO:root:[1:30] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-30.pt\"\n",
      "  7%|▋         | 35/476 [3:12:34<31:26:55, 256.73s/it]INFO:root:[1:35] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-35.pt\"\n",
      "  8%|▊         | 40/476 [3:42:05<31:07:26, 256.99s/it]INFO:root:[1:40] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-40.pt\"\n",
      "  9%|▉         | 45/476 [4:11:37<30:46:24, 257.04s/it]INFO:root:[1:45] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-45.pt\"\n",
      " 11%|█         | 50/476 [4:41:10<30:27:47, 257.43s/it]INFO:root:[2:50] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-50.pt\"\n",
      " 12%|█▏        | 55/476 [5:10:43<30:05:47, 257.36s/it]INFO:root:[2:55] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-55.pt\"\n",
      " 13%|█▎        | 60/476 [5:40:18<29:44:38, 257.40s/it]INFO:root:[2:60] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-60.pt\"\n",
      " 14%|█▎        | 65/476 [6:09:51<29:22:00, 257.23s/it]INFO:root:[2:65] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-65.pt\"\n",
      " 15%|█▍        | 70/476 [6:39:27<29:04:53, 257.87s/it]INFO:root:[2:70] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-70.pt\"\n",
      " 16%|█▌        | 75/476 [7:09:03<28:43:31, 257.88s/it]INFO:root:[3:75] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-75.pt\"\n",
      " 17%|█▋        | 80/476 [7:38:39<28:20:05, 257.59s/it]INFO:root:[3:80] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-80.pt\"\n",
      " 18%|█▊        | 85/476 [8:08:12<27:56:11, 257.22s/it]INFO:root:[3:85] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-85.pt\"\n",
      " 19%|█▉        | 90/476 [8:37:44<27:34:22, 257.16s/it]INFO:root:[3:90] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-90.pt\"\n",
      " 20%|█▉        | 95/476 [9:07:17<27:14:14, 257.36s/it]INFO:root:[3:95] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-95.pt\"\n",
      " 21%|██        | 100/476 [9:36:48<26:51:03, 257.08s/it]INFO:root:[4:100] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-100.pt\"\n",
      " 22%|██▏       | 105/476 [10:06:19<26:29:01, 256.99s/it]INFO:root:[4:105] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-105.pt\"\n",
      " 23%|██▎       | 110/476 [10:35:50<26:07:24, 256.95s/it]INFO:root:[4:110] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-110.pt\"\n",
      " 24%|██▍       | 115/476 [11:05:22<25:47:19, 257.17s/it]INFO:root:[4:115] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-115.pt\"\n",
      " 25%|██▌       | 120/476 [11:34:52<25:25:52, 257.17s/it]INFO:root:[5:120] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-120.pt\"\n",
      " 26%|██▋       | 125/476 [12:04:23<25:03:26, 257.00s/it]INFO:root:[5:125] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-125.pt\"\n",
      " 27%|██▋       | 130/476 [12:33:54<24:41:45, 256.95s/it]INFO:root:[5:130] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-130.pt\"\n",
      " 28%|██▊       | 135/476 [13:03:25<24:20:07, 256.91s/it]INFO:root:[5:135] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-135.pt\"\n",
      " 29%|██▉       | 140/476 [13:32:59<24:01:12, 257.36s/it]INFO:root:[5:140] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-140.pt\"\n",
      " 30%|███       | 145/476 [14:02:28<23:37:54, 257.02s/it]INFO:root:[6:145] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-145.pt\"\n",
      " 32%|███▏      | 150/476 [14:31:57<23:15:07, 256.77s/it]INFO:root:[6:150] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-150.pt\"\n",
      " 33%|███▎      | 155/476 [15:01:28<22:53:46, 256.78s/it]INFO:root:[6:155] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-155.pt\"\n",
      " 34%|███▎      | 160/476 [15:30:55<22:31:17, 256.58s/it]INFO:root:[6:160] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-160.pt\"\n",
      " 35%|███▍      | 165/476 [16:00:25<22:10:31, 256.69s/it]INFO:root:[6:165] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-165.pt\"\n",
      " 36%|███▌      | 170/476 [16:29:54<21:49:37, 256.79s/it]INFO:root:[7:170] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-170.pt\"\n",
      " 37%|███▋      | 175/476 [16:59:21<21:27:01, 256.55s/it]INFO:root:[7:175] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-175.pt\"\n",
      " 38%|███▊      | 180/476 [17:28:50<21:05:59, 256.62s/it]INFO:root:[7:180] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-180.pt\"\n",
      " 39%|███▉      | 185/476 [17:58:17<20:44:15, 256.55s/it]INFO:root:[7:185] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-185.pt\"\n",
      " 40%|███▉      | 190/476 [18:27:47<20:23:40, 256.72s/it]INFO:root:[7:190] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-190.pt\"\n",
      " 41%|████      | 195/476 [18:57:16<20:02:21, 256.73s/it]INFO:root:[8:195] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-195.pt\"\n",
      " 42%|████▏     | 200/476 [19:26:43<19:39:33, 256.43s/it]INFO:root:[8:200] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-200.pt\"\n",
      " 43%|████▎     | 205/476 [19:56:09<19:17:57, 256.37s/it]INFO:root:[8:205] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-205.pt\"\n",
      " 44%|████▍     | 210/476 [20:25:35<18:55:51, 256.21s/it]INFO:root:[8:210] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-210.pt\"\n",
      " 45%|████▌     | 215/476 [20:55:03<18:35:51, 256.52s/it]INFO:root:[9:215] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-215.pt\"\n",
      " 46%|████▌     | 220/476 [21:24:30<18:14:58, 256.64s/it]INFO:root:[9:220] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-220.pt\"\n",
      " 47%|████▋     | 225/476 [21:53:56<17:52:31, 256.38s/it]INFO:root:[9:225] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-225.pt\"\n",
      " 48%|████▊     | 230/476 [22:23:24<17:31:54, 256.56s/it]INFO:root:[9:230] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-230.pt\"\n",
      " 49%|████▉     | 235/476 [22:52:51<17:09:53, 256.40s/it]INFO:root:[9:235] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-235.pt\"\n",
      " 50%|█████     | 240/476 [23:22:20<16:49:12, 256.58s/it]INFO:root:[10:240] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-240.pt\"\n",
      " 51%|█████▏    | 245/476 [23:51:59<16:35:01, 258.45s/it]INFO:root:[10:245] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-245.pt\"\n",
      " 53%|█████▎    | 250/476 [24:21:44<16:15:49, 259.07s/it]INFO:root:[10:250] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-250.pt\"\n",
      " 54%|█████▎    | 255/476 [24:51:32<15:55:50, 259.51s/it]INFO:root:[10:255] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-255.pt\"\n",
      " 55%|█████▍    | 260/476 [25:21:19<15:34:14, 259.51s/it]INFO:root:[10:260] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-260.pt\"\n",
      " 56%|█████▌    | 265/476 [25:51:08<15:13:18, 259.71s/it]INFO:root:[11:265] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-265.pt\"\n",
      " 57%|█████▋    | 270/476 [26:20:56<14:51:24, 259.64s/it]INFO:root:[11:270] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-270.pt\"\n",
      " 58%|█████▊    | 275/476 [26:50:46<14:30:17, 259.79s/it]INFO:root:[11:275] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-275.pt\"\n",
      " 59%|█████▉    | 280/476 [27:20:35<14:09:03, 259.92s/it]INFO:root:[11:280] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-280.pt\"\n",
      " 60%|█████▉    | 285/476 [27:50:25<13:47:03, 259.81s/it]INFO:root:[11:285] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-285.pt\"\n",
      " 61%|██████    | 290/476 [28:20:16<13:25:52, 259.96s/it]INFO:root:[12:290] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-290.pt\"\n",
      " 62%|██████▏   | 295/476 [28:50:02<13:02:44, 259.47s/it]INFO:root:[12:295] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-295.pt\"\n",
      " 63%|██████▎   | 300/476 [29:19:50<12:41:17, 259.53s/it]INFO:root:[12:300] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-300.pt\"\n",
      " 64%|██████▍   | 305/476 [29:49:34<12:18:47, 259.22s/it]INFO:root:[12:305] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-305.pt\"\n",
      " 65%|██████▌   | 310/476 [30:19:23<11:58:48, 259.81s/it]INFO:root:[13:310] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-310.pt\"\n",
      " 66%|██████▌   | 315/476 [30:49:14<11:37:44, 260.03s/it]INFO:root:[13:315] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-315.pt\"\n",
      " 67%|██████▋   | 320/476 [31:19:02<11:14:51, 259.56s/it]INFO:root:[13:320] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-320.pt\"\n",
      " 68%|██████▊   | 325/476 [31:48:46<10:51:53, 259.03s/it]INFO:root:[13:325] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-325.pt\"\n",
      " 69%|██████▉   | 330/476 [32:18:33<10:31:57, 259.71s/it]INFO:root:[13:330] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-10/step-330.pt\"\n",
      " 70%|███████   | 335/476 [32:48:19<10:09:12, 259.23s/it]"
     ]
    }
   ],
   "source": [
    "step_num = 0\n",
    "steps_since_last_eval = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        outputs = model(to_device(batch, DEVICE))\n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "        train_loss += loss.item()\n",
    "        new_scores = eval_batch(batch[\"label\"], outputs)\n",
    "        train_scores = sum_scores(train_scores, new_scores)\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS: \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / GRAD_ACCUM_STEPS, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            steps_since_last_eval += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        if steps_since_last_eval == STEPS_BETWEEN_EVAL: # TODO: add 0-th step\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    outputs = model(to_device(batch, DEVICE))\n",
    "                    loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    new_scores = eval_batch(batch[\"label\"], outputs)\n",
    "                    val_scores = sum_scores(val_scores, new_scores)\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss / val_l, step_num)            \n",
    "            for k, v in div_scores(val_scores, val_l).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_eval = 0\n",
    "        \n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            logging.info(f\"[{epoch_num}:{step_num}] Saving checkpoint to \\\"{str(p)}\\\"\")\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's load the best checkpoint according to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_NUM = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = Classifier(blip_model).to(DEVICE)\n",
    "checkpoint.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-{CHECKPOINT_NUM}.pt\"))\n",
    "checkpoint.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (i, batch) in enumerate(tqdm(test_dl)):\n",
    "        preds = checkpoint(to_device(batch, DEVICE))[0].numpy(force=True)\n",
    "        row = test_df.iloc[i]\n",
    "        predictions.append({row[f\"image{j}\"]: preds[j] for j in range(10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (i, batch) in enumerate(tqdm(test_dl)):\n",
    "        preds = checkpoint(to_device(batch, DEVICE))[0].numpy(force=True)\n",
    "        row = test_df.iloc[i]\n",
    "        predictions.append({row[f\"image{j}\"]: preds[j] for j in range(10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    test_df.iloc[:, 2:-1].values,\n",
    "    test_df[\"label\"].values.reshape(-1, 1),\n",
    "    predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# creates a file in <project root>/data with submissions in target format\n",
    "with open(PATH / f\"blip-{HEAD}-{MODEL_VERSION}-{CHECKPOINT_NUM}_submission.json\", 'w') as f:\n",
    "    json.dump([{k: str(v) for k, v in p.items()} for p in predictions], f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
