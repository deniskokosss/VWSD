{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP finetuning to target task\n",
    "\n",
    "TODO: rewrite this to reflect the latest changes\n",
    "\n",
    "Sample is formed from a single row of dataset:\n",
    "$$\\operatorname{batch} = ((E_t, E_{i_0}), (E_t, E_{i_1}), ..., (E_t, E_{i_9})); \\operatorname{batch} : R^{10 \\times (E_t + E_i)}$$\n",
    "ITM predicts probas for $y = 0$, $y = 1$\n",
    "$$\\operatorname{ITM} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10 \\times 2}$$\n",
    "Model is defined as:\n",
    "$$\\operatorname{F} = \\operatorname{softmax} \\circ \\operatorname{ITM} \\circ \\operatorname{batch}$$\n",
    "$$\\operatorname{F} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10}$$\n",
    "So, this definition is for a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "import torch.nn as nn\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.utils import evaluate, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD = \"itm\" # \"itm\" | \"itc\" | \"mean\"\n",
    "MODEL_VERSION = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"checkpoints\").resolve() / f\"BLIP-{HEAD}-{MODEL_VERSION}\"\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "# WARNING: this is specific to my setup\n",
    "DEVICE = torch.device(\"cuda:0\") # 3090\n",
    "# DEVICE = torch.device(\"cuda:1\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 32\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLIP_VARIANT = \"large\" # \"base\" | \"large\"\n",
    "NUM_EPOCHS = 10\n",
    "NUM_PICS = 10\n",
    "WARMUP_STEPS_FRAC = 0.1\n",
    "STEPS_BETWEEN_EVAL = 5\n",
    "GRAD_ACCUM_STEPS = 128\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_EVAL\n",
    "LR = 5e-6\n",
    "WEIGHT_DECAY = 0.001\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALIDATION_BATCH_SIZE = 8\n",
    "HEAD_SUM_BIAS_ENABLED = True\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM_STEPS * TRAIN_BATCH_SIZE\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_repeat(value):\n",
    "    while True:\n",
    "        yield value\n",
    "\n",
    "def concat_iters(*iterables):\n",
    "    for it in iterables:\n",
    "        for value in it:\n",
    "            yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        images_path: Path,\n",
    "        text_processor,\n",
    "        vis_processor,\n",
    "        use_context_as_text: bool = True,\n",
    "        enable_cache: bool = False,\n",
    "    ) -> None:\n",
    "        self.df = df\n",
    "        self.images_path = images_path\n",
    "        self.text_processor = text_processor\n",
    "        self.vis_processor = vis_processor\n",
    "        self.tokens_cache = dict()\n",
    "        self.image_tensor_cache = dict()\n",
    "        self.enable_cache = enable_cache\n",
    "        self.text_field = \"context\" if use_context_as_text else \"word\"\n",
    "        self.labels_map = self._gen_labels()\n",
    "\n",
    "    def _gen_labels(self) -> Dict[int, int]: # index to label\n",
    "        labels = self.df[\"label\"].values\n",
    "        zips = []\n",
    "        for i in range(NUM_PICS):\n",
    "            images = self.df[f\"image{i}\"].values\n",
    "            zips.append(zip(np.argwhere(labels == images).reshape(-1), infinite_repeat(i)))\n",
    "        return dict(concat_iters(*tuple(zips)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _make_image_tensor(self, name: str) -> torch.Tensor:\n",
    "        return self.vis_processor(Image.open(self.images_path / name).convert(\"RGB\"))\n",
    "\n",
    "    def _get_image_tensor(self, name: str) -> Image:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_image_tensor(name)\n",
    "        if name in self.image_tensor_cache:\n",
    "            return self.image_tensor_cache[name]\n",
    "        t = self._make_image_tensor(name)\n",
    "        self.image_tensor_cache[name] = t\n",
    "        return t\n",
    "\n",
    "    def _get_image_batch(self, idx: int) -> torch.Tensor:\n",
    "        row = self.df.iloc[idx]\n",
    "        return torch.stack([self._get_image_tensor(row[f\"image{i}\"]) for i in range(NUM_PICS)])\n",
    "\n",
    "    def _make_tokens(self, idx: int) -> BatchEncoding:\n",
    "        return self.text_processor(self.df.iloc[idx][self.text_field])\n",
    "    \n",
    "    def _get_tokens(self, idx: int) -> BatchEncoding:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_tokens(idx)\n",
    "        if idx in self.tokens_cache:\n",
    "            return self.tokens_cache[idx]\n",
    "        t = self._make_tokens(idx)\n",
    "        self.tokens_cache[idx] = t\n",
    "        return t\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, BatchEncoding, int]]:\n",
    "        # makes a batch for each row!\n",
    "        return {\n",
    "            \"text\": self._get_tokens(idx),\n",
    "            \"images\": self._get_image_batch(idx),\n",
    "            \"label\": self.labels_map[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ItmDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val_ds = ItmDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "test_ds = ItmDataset(\n",
    "    df=test_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "train_l = len(train_dl)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_l = len(val_dl)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=1, num_workers=NUM_WORKERS, persistent_workers=True, shuffle=False)\n",
    "test_l = len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(object, device):\n",
    "    if not isinstance(object, dict):\n",
    "        raise NotImplementedError(\"Implement other types than dict if needed!\")\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in object.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        blip_model: BlipBase,\n",
    "        match_head: str = \"itm\",\n",
    "        head_sum_bias_enabled: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.blip_model = blip_model\n",
    "        self.match_head = match_head\n",
    "        if self.match_head == \"mean\":\n",
    "            self.head_combiner = nn.Linear(2, 1, bias=head_sum_bias_enabled)\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # TODO: move all this batch-dependent stuff to collate_fn?\n",
    "        # TODO: optimize!\n",
    "        # text: str\n",
    "        # image: \n",
    "        images_shape = inputs[\"images\"].shape # image: (B, NUM_PICS, C, H, W)\n",
    "        batch_size = images_shape[0]\n",
    "        text_input = []\n",
    "        for t in inputs[\"text\"]:\n",
    "            for _ in range(NUM_PICS):\n",
    "                text_input.append(t)\n",
    "        images_input = inputs[\"images\"].reshape(batch_size * NUM_PICS, images_shape[2], images_shape[3], images_shape[4]) # image: (B * NUM_PICS, C, H, W)\n",
    "        # (B * X, 2)\n",
    "        if self.match_head == \"itm\":\n",
    "            batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=self.match_head).reshape(batch_size, NUM_PICS, 2) # (B, NUM_PICS, 2)\n",
    "            batch_probas = F.softmax(batch_outputs[:, :, 1], dim=1)\n",
    "        elif self.match_head == \"itc\":\n",
    "            batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=self.match_head).reshape(batch_size, NUM_PICS) # (B * NUM_PICS) -> (B, NUM_PICS)\n",
    "            # hugginface VisionTextEncoder see cos * N before softmax\n",
    "            # TODO: N as hyperparam const\n",
    "            # TODO: or learnable param\n",
    "            batch_probas = F.softmax(batch_outputs, dim=1) # softmax(cosine similarity) => =(\n",
    "        elif self.match_head == \"mean\":\n",
    "            raise NotImplementedError(\"Implement me!\")\n",
    "            # Warning: not tested\n",
    "            # TODO: replace with mean(p_itm, p_itc)\n",
    "            # itm_batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=\"itm\").reshape(batch_size, 10, 2)\n",
    "            # itc_batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=\"itc\").reshape(batch_size, 10)\n",
    "            # dual_probas = torch.stack([\n",
    "            #     torch.stack([F.softmax(itm_batch_outputs[i, :, 1], dim=0) for i in range(batch_size)]).reshape(batch_size * 10),\n",
    "            #     torch.stack([F.softmax(batch_outputs[i, :], dim=0) for i in range(batch_size)]).reshape(batch_size * 10),\n",
    "            # ], dim=1)\n",
    "            # batch_ouputs = F.softmax(self.head_combiner(dual_probas), dim=0).reshape(batch_size, 10)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected value for match_head parameter \\\"{self.match_head}\\\". Allowed values: \\\"itm\\\", \\\"itc\\\" or \\\"mean\\\".\")\n",
    "        return batch_probas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2name = {\n",
    "    \"acc1\": \"Accuracy@Top1\",\n",
    "    \"acc3\": \"Accuracy@Top3\",\n",
    "    \"mrr\": \"Mean Reciprocal Rank\",\n",
    "}\n",
    "\n",
    "labels_range = np.arange(NUM_PICS)\n",
    "\n",
    "def eval_batch(labels, preds):\n",
    "    labels = labels.numpy(force=True)\n",
    "    preds = preds.numpy(force=True)\n",
    "    return {\n",
    "        \"acc1\": top_k_accuracy_score(labels, preds, k=1, labels=labels_range), \n",
    "        \"acc3\": top_k_accuracy_score(labels, preds, k=3, labels=labels_range),\n",
    "        \"mrr\": mrr(labels, preds),\n",
    "    }\n",
    "\n",
    "def sum_scores(scores, new_scores):\n",
    "    return {k: scores[k] + new_scores[k] for k in scores}\n",
    "\n",
    "def div_scores(scores, n):\n",
    "    return {k: v / n for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476 training steps which include 47 warmup ones\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "num_training_steps = int(NUM_EPOCHS * (train_l / GRAD_ACCUM_STEPS))\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_STEPS_FRAC)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement validation of untuned model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/476 [08:43<13:40:57, 104.58s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "INFO:root:[0:5] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-5.pt\"\n",
      "  2%|▏         | 10/476 [33:20<22:41:33, 175.31s/it]INFO:root:[0:10] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-10.pt\"\n",
      "  3%|▎         | 15/476 [57:56<23:43:29, 185.27s/it]INFO:root:[0:15] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-15.pt\"\n",
      "  4%|▎         | 17/476 [1:17:19<43:57:14, 344.74s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  4%|▍         | 19/476 [1:20:49<28:12:36, 222.22s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  4%|▍         | 20/476 [1:22:33<23:40:42, 186.93s/it]INFO:root:[0:20] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-20.pt\"\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  5%|▌         | 25/476 [1:47:08<23:25:47, 187.02s/it]INFO:root:[0:25] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-25.pt\"\n",
      "  6%|▋         | 30/476 [2:11:46<23:12:06, 187.28s/it]INFO:root:[0:30] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-30.pt\"\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  7%|▋         | 35/476 [2:36:23<22:56:16, 187.25s/it]INFO:root:[0:35] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-35.pt\"\n",
      "  8%|▊         | 40/476 [3:01:00<22:40:56, 187.29s/it]INFO:root:[0:40] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-40.pt\"\n",
      "  9%|▉         | 44/476 [3:23:51<26:42:00, 222.50s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  9%|▉         | 45/476 [3:25:36<22:24:13, 187.13s/it]INFO:root:[0:45] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-45.pt\"\n",
      " 11%|█         | 50/476 [3:50:12<22:09:13, 187.21s/it]INFO:root:[1:50] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-50.pt\"\n",
      " 12%|█▏        | 55/476 [4:14:48<21:53:04, 187.14s/it]INFO:root:[1:55] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-55.pt\"\n",
      " 13%|█▎        | 60/476 [4:39:23<21:37:23, 187.12s/it]INFO:root:[1:60] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-60.pt\"\n",
      " 14%|█▎        | 65/476 [5:04:00<21:22:26, 187.22s/it]INFO:root:[1:65] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-65.pt\"\n",
      " 15%|█▍        | 70/476 [5:28:35<21:05:43, 187.05s/it]INFO:root:[1:70] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-70.pt\"\n",
      " 16%|█▌        | 75/476 [5:53:10<20:50:07, 187.05s/it]INFO:root:[1:75] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-75.pt\"\n",
      " 17%|█▋        | 80/476 [6:17:45<20:34:38, 187.07s/it]INFO:root:[1:80] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-80.pt\"\n",
      " 18%|█▊        | 85/476 [6:42:21<20:19:18, 187.11s/it]INFO:root:[1:85] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-85.pt\"\n",
      " 19%|█▉        | 90/476 [7:06:58<20:03:56, 187.14s/it]INFO:root:[1:90] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-90.pt\"\n",
      " 20%|█▉        | 95/476 [7:31:33<19:48:14, 187.13s/it]INFO:root:[1:95] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-95.pt\"\n",
      " 21%|██        | 100/476 [7:56:10<19:33:18, 187.23s/it]INFO:root:[2:100] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-100.pt\"\n",
      " 22%|██▏       | 105/476 [8:20:46<19:17:26, 187.19s/it]INFO:root:[2:105] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-105.pt\"\n",
      " 23%|██▎       | 110/476 [8:45:23<19:02:00, 187.21s/it]INFO:root:[2:110] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-110.pt\"\n",
      " 24%|██▍       | 115/476 [9:09:59<18:45:39, 187.09s/it]INFO:root:[2:115] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-115.pt\"\n",
      " 25%|██▌       | 120/476 [9:34:34<18:29:56, 187.07s/it]INFO:root:[2:120] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-120.pt\"\n",
      " 26%|██▋       | 125/476 [9:59:05<18:11:17, 186.55s/it]INFO:root:[2:125] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-125.pt\"\n",
      " 27%|██▋       | 130/476 [10:23:33<17:54:06, 186.26s/it]INFO:root:[2:130] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-130.pt\"\n",
      " 28%|██▊       | 135/476 [10:48:01<17:37:58, 186.16s/it]INFO:root:[2:135] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-135.pt\"\n",
      " 29%|██▉       | 140/476 [11:12:28<17:21:59, 186.07s/it]INFO:root:[2:140] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-140.pt\"\n",
      " 30%|███       | 145/476 [11:36:55<17:06:59, 186.16s/it]INFO:root:[3:145] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-145.pt\"\n",
      " 32%|███▏      | 150/476 [12:01:26<16:52:55, 186.43s/it]INFO:root:[3:150] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-150.pt\"\n",
      " 33%|███▎      | 155/476 [12:25:59<16:38:57, 186.72s/it]INFO:root:[3:155] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-155.pt\"\n",
      " 34%|███▎      | 160/476 [12:50:32<16:23:48, 186.80s/it]INFO:root:[3:160] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-160.pt\"\n",
      " 35%|███▍      | 165/476 [13:15:07<16:09:00, 186.95s/it]INFO:root:[3:165] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-165.pt\"\n",
      " 36%|███▌      | 170/476 [13:39:43<15:52:44, 186.81s/it]INFO:root:[3:170] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-170.pt\"\n",
      " 37%|███▋      | 175/476 [14:04:10<15:34:20, 186.25s/it]INFO:root:[3:175] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-175.pt\"\n",
      " 38%|███▊      | 180/476 [14:28:41<15:20:31, 186.59s/it]INFO:root:[3:180] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-180.pt\"\n",
      " 39%|███▉      | 185/476 [14:53:17<15:07:08, 187.04s/it]INFO:root:[3:185] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-185.pt\"\n",
      " 40%|███▉      | 190/476 [15:17:52<14:51:09, 186.96s/it]INFO:root:[3:190] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-190.pt\"\n",
      " 41%|████      | 195/476 [15:42:22<14:33:22, 186.48s/it]INFO:root:[4:195] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-195.pt\"\n",
      " 42%|████▏     | 200/476 [16:06:54<14:18:45, 186.69s/it]INFO:root:[4:200] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-200.pt\"\n",
      " 43%|████▎     | 205/476 [16:31:23<14:01:00, 186.20s/it]INFO:root:[4:205] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-205.pt\"\n",
      " 44%|████▍     | 210/476 [16:55:47<13:43:40, 185.79s/it]INFO:root:[4:210] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-210.pt\"\n",
      " 45%|████▌     | 215/476 [17:20:11<13:27:52, 185.72s/it]INFO:root:[4:215] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-215.pt\"\n",
      " 46%|████▌     | 220/476 [17:44:35<13:12:22, 185.71s/it]INFO:root:[4:220] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-220.pt\"\n",
      " 47%|████▋     | 225/476 [18:09:00<12:57:06, 185.76s/it]INFO:root:[4:225] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-225.pt\"\n",
      " 48%|████▊     | 230/476 [18:33:25<12:41:55, 185.84s/it]INFO:root:[4:230] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-230.pt\"\n",
      " 49%|████▉     | 235/476 [18:57:50<12:26:16, 185.79s/it]INFO:root:[4:235] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-235.pt\"\n",
      " 50%|█████     | 240/476 [19:22:14<12:10:55, 185.83s/it]INFO:root:[5:240] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-240.pt\"\n",
      " 51%|█████▏    | 245/476 [19:46:38<11:55:04, 185.73s/it]INFO:root:[5:245] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-245.pt\"\n",
      " 53%|█████▎    | 250/476 [20:11:02<11:39:21, 185.67s/it]INFO:root:[5:250] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-250.pt\"\n",
      " 54%|█████▎    | 255/476 [20:35:26<11:23:51, 185.66s/it]INFO:root:[5:255] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-255.pt\"\n",
      " 55%|█████▍    | 260/476 [20:59:50<11:08:32, 185.70s/it]INFO:root:[5:260] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-260.pt\"\n",
      " 56%|█████▌    | 265/476 [21:24:14<10:53:00, 185.69s/it]INFO:root:[5:265] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-265.pt\"\n",
      " 57%|█████▋    | 270/476 [21:48:37<10:37:05, 185.56s/it]INFO:root:[5:270] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-270.pt\"\n",
      " 58%|█████▊    | 275/476 [22:12:57<10:20:41, 185.28s/it]INFO:root:[5:275] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-275.pt\"\n",
      " 59%|█████▉    | 280/476 [22:37:16<10:04:38, 185.09s/it]INFO:root:[5:280] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-280.pt\"\n",
      " 60%|█████▉    | 285/476 [23:01:34<9:48:55, 185.00s/it] INFO:root:[5:285] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-285.pt\"\n",
      " 61%|██████    | 290/476 [23:25:56<9:34:33, 185.34s/it] INFO:root:[6:290] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-290.pt\"\n",
      " 62%|██████▏   | 295/476 [23:50:18<9:19:20, 185.42s/it] INFO:root:[6:295] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-295.pt\"\n",
      " 63%|██████▎   | 300/476 [24:14:43<9:05:22, 185.92s/it] INFO:root:[6:300] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-300.pt\"\n",
      " 64%|██████▍   | 305/476 [24:39:18<8:51:55, 186.64s/it] INFO:root:[6:305] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-305.pt\"\n",
      " 65%|██████▌   | 310/476 [25:03:42<8:33:27, 185.58s/it] INFO:root:[6:310] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-310.pt\"\n",
      " 66%|██████▌   | 315/476 [25:27:55<8:15:04, 184.50s/it] INFO:root:[6:315] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-315.pt\"\n",
      " 67%|██████▋   | 320/476 [25:52:07<7:58:46, 184.15s/it] INFO:root:[6:320] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-320.pt\"\n",
      " 68%|██████▊   | 325/476 [26:16:18<7:43:17, 184.09s/it] INFO:root:[6:325] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-325.pt\"\n",
      " 69%|██████▉   | 330/476 [26:40:29<7:27:55, 184.08s/it] INFO:root:[6:330] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-330.pt\"\n",
      " 70%|███████   | 335/476 [27:04:41<7:12:45, 184.15s/it] INFO:root:[7:335] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-335.pt\"\n",
      " 71%|███████▏  | 340/476 [27:28:52<6:57:00, 183.97s/it] INFO:root:[7:340] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-340.pt\"\n",
      " 72%|███████▏  | 345/476 [27:53:03<6:41:38, 183.96s/it] INFO:root:[7:345] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-345.pt\"\n",
      " 74%|███████▎  | 350/476 [28:17:13<6:26:13, 183.92s/it] INFO:root:[7:350] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-350.pt\"\n",
      " 75%|███████▍  | 355/476 [28:41:23<6:10:57, 183.94s/it] INFO:root:[7:355] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-355.pt\"\n",
      " 76%|███████▌  | 360/476 [29:05:35<5:55:56, 184.11s/it] INFO:root:[7:360] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-360.pt\"\n",
      " 77%|███████▋  | 365/476 [29:29:46<5:40:32, 184.08s/it] INFO:root:[7:365] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-365.pt\"\n",
      " 78%|███████▊  | 370/476 [29:53:57<5:25:05, 184.01s/it] INFO:root:[7:370] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-370.pt\"\n",
      " 79%|███████▉  | 375/476 [30:18:08<5:09:40, 183.96s/it] INFO:root:[7:375] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-375.pt\"\n",
      " 80%|███████▉  | 380/476 [30:42:17<4:54:15, 183.91s/it] INFO:root:[7:380] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-380.pt\"\n",
      " 81%|████████  | 385/476 [31:06:30<4:39:18, 184.16s/it] INFO:root:[8:385] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-385.pt\"\n",
      " 82%|████████▏ | 390/476 [31:30:45<4:24:21, 184.44s/it] INFO:root:[8:390] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-390.pt\"\n",
      " 83%|████████▎ | 395/476 [31:55:00<4:09:08, 184.55s/it] INFO:root:[8:395] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-395.pt\"\n",
      " 84%|████████▍ | 400/476 [32:19:17<3:53:52, 184.64s/it]INFO:root:[8:400] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-400.pt\"\n",
      " 85%|████████▌ | 405/476 [32:43:30<3:38:08, 184.34s/it]INFO:root:[8:405] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-405.pt\"\n",
      " 86%|████████▌ | 410/476 [33:07:42<3:22:29, 184.08s/it]INFO:root:[8:410] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-410.pt\"\n",
      " 87%|████████▋ | 415/476 [33:31:54<3:07:18, 184.24s/it]INFO:root:[8:415] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-415.pt\"\n",
      " 88%|████████▊ | 420/476 [33:56:07<2:51:51, 184.14s/it]INFO:root:[8:420] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-420.pt\"\n",
      " 89%|████████▉ | 425/476 [34:20:18<2:36:22, 183.97s/it]INFO:root:[8:425] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-425.pt\"\n",
      " 90%|█████████ | 430/476 [34:44:29<2:21:07, 184.08s/it]INFO:root:[9:430] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-430.pt\"\n",
      " 91%|█████████▏| 435/476 [35:08:38<2:05:38, 183.86s/it]INFO:root:[9:435] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-435.pt\"\n",
      " 92%|█████████▏| 440/476 [35:32:49<1:50:23, 183.99s/it]INFO:root:[9:440] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-440.pt\"\n",
      " 93%|█████████▎| 445/476 [35:56:59<1:34:58, 183.84s/it]INFO:root:[9:445] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-445.pt\"\n",
      " 95%|█████████▍| 450/476 [36:21:08<1:19:36, 183.72s/it]INFO:root:[9:450] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-450.pt\"\n",
      " 96%|█████████▌| 455/476 [36:45:18<1:04:19, 183.80s/it]INFO:root:[9:455] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-455.pt\"\n",
      " 97%|█████████▋| 460/476 [37:09:27<48:59, 183.73s/it]  INFO:root:[9:460] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-460.pt\"\n",
      " 98%|█████████▊| 465/476 [37:33:36<33:41, 183.80s/it]  INFO:root:[9:465] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-465.pt\"\n",
      " 99%|█████████▊| 470/476 [37:57:47<18:24, 184.05s/it]  INFO:root:[9:470] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-470.pt\"\n",
      "100%|█████████▉| 475/476 [38:21:58<03:03, 183.92s/it]INFO:root:[9:475] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-11/step-475.pt\"\n",
      "100%|██████████| 476/476 [38:39:17<00:00, 440.46s/it]"
     ]
    }
   ],
   "source": [
    "step_num = 0\n",
    "steps_since_last_eval = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        outputs = model(to_device(batch, DEVICE))\n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "        train_loss += loss.item()\n",
    "        new_scores = eval_batch(batch[\"label\"], outputs)\n",
    "        train_scores = sum_scores(train_scores, new_scores)\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS: \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / GRAD_ACCUM_STEPS, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            steps_since_last_eval += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        if steps_since_last_eval == STEPS_BETWEEN_EVAL: # TODO: add 0-th step\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    outputs = model(to_device(batch, DEVICE))\n",
    "                    loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    new_scores = eval_batch(batch[\"label\"], outputs)\n",
    "                    val_scores = sum_scores(val_scores, new_scores)\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss / val_l, step_num)            \n",
    "            for k, v in div_scores(val_scores, val_l).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_eval = 0\n",
    "        \n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            logging.info(f\"[{epoch_num}:{step_num}] Saving checkpoint to \\\"{str(p)}\\\"\")\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's load the best checkpoint according to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_NUM = 475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (blip_model): BlipITM(\n",
       "    (text_encoder): XBertEncoder(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (visual_encoder): VisionTransformerEncoder(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.004)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.009)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.013)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.017)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.022)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.026)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.030)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.035)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.039)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.043)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.048)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.052)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.057)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.061)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.065)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.070)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.074)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.078)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.083)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.087)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.091)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.096)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): Block(\n",
       "          (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (text_proj): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (itm_head): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = Classifier(blip_model).to(DEVICE)\n",
    "checkpoint.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-{CHECKPOINT_NUM}.pt\"))\n",
    "checkpoint.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 3356/3356 [16:10<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (i, batch) in enumerate(tqdm(test_dl)):\n",
    "        preds = checkpoint(to_device(batch, DEVICE))[0].numpy(force=True)\n",
    "        row = test_df.iloc[i]\n",
    "        predictions.append({row[f\"image{j}\"]: preds[j] for j in range(10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc1': 0.8533969010727056,\n",
       " 'acc3': 0.9755661501787842,\n",
       " 'mrr': 0.9144679276160205}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    test_df.iloc[:, 2:-1].values,\n",
    "    test_df[\"label\"].values.reshape(-1, 1),\n",
    "    predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a file in <project root>/data with submissions in target format\n",
    "with open(PATH / f\"blip-{HEAD}-{MODEL_VERSION}-{CHECKPOINT_NUM}_submission.json\", 'w') as f:\n",
    "    json.dump([{k: str(v) for k, v in p.items()} for p in predictions], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
