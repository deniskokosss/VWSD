{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP finetuning to target task\n",
    "\n",
    "TODO: rewrite this to reflect the latest changes\n",
    "\n",
    "Sample is formed from a single row of dataset:\n",
    "$$\\operatorname{batch} = ((E_t, E_{i_0}), (E_t, E_{i_1}), ..., (E_t, E_{i_9})); \\operatorname{batch} : R^{10 \\times (E_t + E_i)}$$\n",
    "ITM predicts probas for $y = 0$, $y = 1$\n",
    "$$\\operatorname{ITM} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10 \\times 2}$$\n",
    "Model is defined as:\n",
    "$$\\operatorname{F} = \\operatorname{softmax} \\circ \\operatorname{ITM} \\circ \\operatorname{batch}$$\n",
    "$$\\operatorname{F} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10}$$\n",
    "So, this definition is for a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "import torch.nn as nn\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.utils import evaluate, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD = \"itm\" # \"itm\" | \"itc\" | \"mean\"\n",
    "MODEL_VERSION = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"checkpoints\").resolve() / f\"BLIP-{HEAD}-{MODEL_VERSION}\"\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:1\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "# WARNING: this is specific to my setup\n",
    "DEVICE = torch.device(\"cuda:1\") # 3060\n",
    "# DEVICE = torch.device(\"cuda:1\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 32\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\"\n",
    "NUM_EPOCHS = 15\n",
    "NUM_PICS = 10\n",
    "WARMUP_STEPS_FRAC = 0.1\n",
    "STEPS_BETWEEN_EVAL = 100\n",
    "GRAD_ACCUM_STEPS = 15\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_EVAL\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.001\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALIDATION_BATCH_SIZE = 3\n",
    "HEAD_SUM_BIAS_ENABLED = True\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM_STEPS * TRAIN_BATCH_SIZE\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_repeat(value):\n",
    "    while True:\n",
    "        yield value\n",
    "\n",
    "def concat_iters(*iterables):\n",
    "    for it in iterables:\n",
    "        for value in it:\n",
    "            yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        images_path: Path,\n",
    "        text_processor,\n",
    "        vis_processor,\n",
    "        use_context_as_text: bool = True,\n",
    "        enable_cache: bool = False,\n",
    "    ) -> None:\n",
    "        self.df = df\n",
    "        self.images_path = images_path\n",
    "        self.text_processor = text_processor\n",
    "        self.vis_processor = vis_processor\n",
    "        self.tokens_cache = dict()\n",
    "        self.image_tensor_cache = dict()\n",
    "        self.enable_cache = enable_cache\n",
    "        self.text_field = \"context\" if use_context_as_text else \"word\"\n",
    "        self.labels_map = self._gen_labels()\n",
    "\n",
    "    def _gen_labels(self) -> Dict[int, int]: # index to label\n",
    "        labels = self.df[\"label\"].values\n",
    "        zips = []\n",
    "        for i in range(NUM_PICS):\n",
    "            images = self.df[f\"image{i}\"].values\n",
    "            zips.append(zip(np.argwhere(labels == images).reshape(-1), infinite_repeat(i)))\n",
    "        return dict(concat_iters(*tuple(zips)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _make_image_tensor(self, name: str) -> torch.Tensor:\n",
    "        return self.vis_processor(Image.open(self.images_path / name).convert(\"RGB\"))\n",
    "\n",
    "    def _get_image_tensor(self, name: str) -> Image:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_image_tensor(name)\n",
    "        if name in self.image_tensor_cache:\n",
    "            return self.image_tensor_cache[name]\n",
    "        t = self._make_image_tensor(name)\n",
    "        self.image_tensor_cache[name] = t\n",
    "        return t\n",
    "\n",
    "    def _get_image_batch(self, idx: int) -> torch.Tensor:\n",
    "        row = self.df.iloc[idx]\n",
    "        return torch.stack([self._get_image_tensor(row[f\"image{i}\"]) for i in range(NUM_PICS)])\n",
    "\n",
    "    def _make_tokens(self, idx: int) -> BatchEncoding:\n",
    "        return self.text_processor(self.df.iloc[idx][self.text_field])\n",
    "    \n",
    "    def _get_tokens(self, idx: int) -> BatchEncoding:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_tokens(idx)\n",
    "        if idx in self.tokens_cache:\n",
    "            return self.tokens_cache[idx]\n",
    "        t = self._make_tokens(idx)\n",
    "        self.tokens_cache[idx] = t\n",
    "        return t\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, BatchEncoding, int]]:\n",
    "        # makes a batch for each row!\n",
    "        return {\n",
    "            \"text\": self._get_tokens(idx),\n",
    "            \"images\": self._get_image_batch(idx),\n",
    "            \"label\": self.labels_map[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ItmDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val_ds = ItmDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "test_ds = ItmDataset(\n",
    "    df=test_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "train_l = len(train_dl)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_l = len(val_dl)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=1, num_workers=NUM_WORKERS, persistent_workers=True, shuffle=False)\n",
    "test_l = len(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(object, device):\n",
    "    if not isinstance(object, dict):\n",
    "        raise NotImplementedError(\"Implement other types than dict if needed!\")\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in object.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        blip_model: BlipBase,\n",
    "        match_head: str = \"itm\",\n",
    "        head_sum_bias_enabled: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.blip_model = blip_model\n",
    "        self.match_head = match_head\n",
    "        if self.match_head == \"mean\":\n",
    "            self.head_combiner = nn.Linear(2, 1, bias=head_sum_bias_enabled)\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        # TODO: move all this batch-dependent stuff to collate_fn?\n",
    "        # TODO: optimize!\n",
    "        # text: str\n",
    "        # image: \n",
    "        images_shape = inputs[\"images\"].shape # image: (B, NUM_PICS, C, H, W)\n",
    "        batch_size = images_shape[0]\n",
    "        text_input = []\n",
    "        for t in inputs[\"text\"]:\n",
    "            for _ in range(NUM_PICS):\n",
    "                text_input.append(t)\n",
    "        # TODO: 10 -> X\n",
    "        images_input = inputs[\"images\"].reshape(batch_size * NUM_PICS, images_shape[2], images_shape[3], images_shape[4]) # image: (B * NUM_PICS, C, H, W)\n",
    "        # (B * X, 2)\n",
    "        if self.match_head == \"itm\":\n",
    "            batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=self.match_head).reshape(batch_size, NUM_PICS, 2) # (B, NUM_PICS, 2)\n",
    "            batch_probas = F.softmax(batch_outputs[:, :, 1], dim=1)\n",
    "        elif self.match_head == \"itc\":\n",
    "            batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=self.match_head).reshape(batch_size, NUM_PICS) # (B * NUM_PICS) -> (B, NUM_PICS)\n",
    "            # hugginface VisionTextEncoder see cos * N before softmax\n",
    "            # TODO: N as hyperparam const\n",
    "            # TODO: or learnable param\n",
    "            batch_probas = F.softmax(batch_outputs, dim=1) # softmax(cosine similarity) => =(\n",
    "        elif self.match_head == \"mean\":\n",
    "            raise NotImplementedError(\"Implement me!\")\n",
    "            # Warning: not tested\n",
    "            # TODO: replace with mean(p_itm, p_itc)\n",
    "            # itm_batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=\"itm\").reshape(batch_size, 10, 2)\n",
    "            # itc_batch_outputs = self.blip_model({\"text_input\": text_input, \"image\": images_input}, match_head=\"itc\").reshape(batch_size, 10)\n",
    "            # dual_probas = torch.stack([\n",
    "            #     torch.stack([F.softmax(itm_batch_outputs[i, :, 1], dim=0) for i in range(batch_size)]).reshape(batch_size * 10),\n",
    "            #     torch.stack([F.softmax(batch_outputs[i, :], dim=0) for i in range(batch_size)]).reshape(batch_size * 10),\n",
    "            # ], dim=1)\n",
    "            # batch_ouputs = F.softmax(self.head_combiner(dual_probas), dim=0).reshape(batch_size, 10)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected value for match_head parameter \\\"{self.match_head}\\\". Allowed values: \\\"itm\\\", \\\"itc\\\" or \\\"mean\\\".\")\n",
    "        return batch_probas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2name = {\n",
    "    \"acc1\": \"Accuracy@Top1\",\n",
    "    \"acc3\": \"Accuracy@Top3\",\n",
    "    \"mrr\": \"Mean Reciprocal Rank\",\n",
    "}\n",
    "\n",
    "labels_range = np.arange(NUM_PICS)\n",
    "\n",
    "def eval_batch(labels, preds):\n",
    "    labels = labels.numpy(force=True)\n",
    "    preds = preds.numpy(force=True)\n",
    "    return {\n",
    "        \"acc1\": top_k_accuracy_score(labels, preds, k=1, labels=labels_range), \n",
    "        \"acc3\": top_k_accuracy_score(labels, preds, k=3, labels=labels_range),\n",
    "        \"mrr\": mrr(labels, preds),\n",
    "    }\n",
    "\n",
    "def sum_scores(scores, new_scores):\n",
    "    return {k: scores[k] + new_scores[k] for k in scores}\n",
    "\n",
    "def div_scores(scores, n):\n",
    "    return {k: v / n for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6103 training steps which include 610 warmup ones\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "num_training_steps = int(NUM_EPOCHS * (train_l / GRAD_ACCUM_STEPS))\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_STEPS_FRAC)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement validation of untuned model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/6103 [18:07<17:59:13, 10.79s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "INFO:root:[0:100] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-7/step-100.pt\"\n",
      "  2%|▏         | 147/6103 [40:51<17:52:30, 10.80s/it]  /home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  3%|▎         | 168/6103 [44:38<17:48:48, 10.81s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  3%|▎         | 173/6103 [45:32<17:47:55, 10.81s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  3%|▎         | 200/6103 [50:24<17:43:07, 10.81s/it]INFO:root:[0:200] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-7/step-200.pt\"\n",
      "  4%|▍         | 262/6103 [1:15:50<17:32:15, 10.81s/it]  /home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  5%|▍         | 300/6103 [1:22:41<17:26:29, 10.82s/it]INFO:root:[0:300] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-7/step-300.pt\"\n",
      "  6%|▌         | 376/6103 [1:50:37<17:11:16, 10.80s/it]  /home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  7%|▋         | 400/6103 [1:54:56<17:07:06, 10.81s/it]INFO:root:[0:400] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-7/step-400.pt\"\n",
      "  8%|▊         | 500/6103 [2:27:13<16:49:08, 10.81s/it]  INFO:root:[1:500] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-7/step-500.pt\"\n",
      " 10%|▉         | 600/6103 [2:59:32<16:32:19, 10.82s/it]  INFO:root:[1:600] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-7/step-600.pt\"\n",
      " 11%|█▏        | 700/6103 [3:31:59<16:21:03, 10.89s/it]  INFO:root:[1:700] Saving checkpoint to \"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-7/step-700.pt\"\n",
      " 13%|█▎        | 800/6103 [4:04:45<16:13:18, 11.01s/it]  "
     ]
    }
   ],
   "source": [
    "step_num = 0\n",
    "steps_since_last_eval = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        outputs = model(to_device(batch, DEVICE))\n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], 10).float().to(DEVICE))\n",
    "        train_loss += loss.item()\n",
    "        new_scores = eval_batch(batch[\"label\"], outputs)\n",
    "        train_scores = sum_scores(train_scores, new_scores)\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS: \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / TRAIN_EFFECTIVE_BATCH_SIZE, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            steps_since_last_eval += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        if steps_since_last_eval == STEPS_BETWEEN_EVAL: # add 0-th step\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    outputs = model(to_device(batch, DEVICE))\n",
    "                    loss = loss_fn(outputs, F.one_hot(batch[\"label\"], 10).float().to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    new_scores = eval_batch(batch[\"label\"], outputs)\n",
    "                    val_scores = sum_scores(val_scores, new_scores)\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss / val_l, step_num)            \n",
    "            for k, v in div_scores(val_scores, val_l).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_eval = 0\n",
    "        \n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            logging.info(f\"[{epoch_num}:{step_num}] Saving checkpoint to \\\"{str(p)}\\\"\")\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's load the best checkpoint according to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_NUM = 6100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Classifier(blip_model).to(DEVICE)\n",
    "checkpoint.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-{CHECKPOINT_NUM}.pt\"))\n",
    "checkpoint.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 3356/3356 [13:34<00:00,  4.12it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (i, batch) in enumerate(tqdm(test_dl)):\n",
    "        preds = checkpoint(to_device(batch, DEVICE))[0].numpy(force=True)\n",
    "        row = test_df.iloc[i]\n",
    "        predictions.append({row[f\"image{j}\"]: preds[j] for j in range(10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc1': 0.8367103694874851,\n",
       " 'acc3': 0.9737783075089392,\n",
       " 'mrr': 0.9047753845280662}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    test_df.iloc[:, 2:-1].values,\n",
    "    test_df[\"label\"].values.reshape(-1, 1),\n",
    "    predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a file in <project root>/data with submissions in target format\n",
    "with open(PATH / f\"blip-{HEAD}-{MODEL_VERSION}-{CHECKPOINT_NUM}_submission.json\", 'w') as f:\n",
    "    json.dump([{k: str(v) for k, v in p.items()} for p in predictions], f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov  4 2022, 16:13:54) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
