{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "import torch.nn as nn\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.utils import evaluate, mrr\n",
    "# from src.itc import ClsITC, ClsITCBatchData, Temperature\n",
    "from src.itm import AltNSDataset, to_device, ITMClassifier, DefaultDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD = \"itm\"\n",
    "MODEL_VERSION = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "VAL2_DATA_PATH = PATH / \"valid2.data.v1.txt\"\n",
    "VAL2_GOLD_PATH = PATH / \"valid2.gold.v1.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "TEST2_DATA_PATH = PATH / \"test2.data.v1.txt\"\n",
    "TEST2_GOLD_PATH = PATH / \"test2.gold.v1.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"checkpoints\").resolve() / f\"BLIP-{HEAD}-{MODEL_VERSION}\" # TODO: maybe add timestamp?\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "NUM_PICS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "writer = SummaryWriter(f\"runs/blip-{HEAD}-{MODEL_VERSION} (ran at {datetime.now()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "# WARNING: this is specific to my setup\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 32\n",
    "PERSISTENT_WORKERS = True\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\"\n",
    "# NUM_NS = 5\n",
    "NUM_EPOCHS = 30\n",
    "WARMUP_STEPS_FRAC = 0.05\n",
    "GRAD_ACCUM_STEPS = 12\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "# cos lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM_STEPS * TRAIN_BATCH_SIZE\n",
    "# NUM_LABELS = NUM_NS + 1\n",
    "NUM_LABELS = NUM_PICS\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_BETWEEN_VAL = 250\n",
    "STEPS_BETWEEN_VAL2 = 500\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_VAL\n",
    "VALIDATION_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = VALIDATION_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "\n",
    "val2_df = pd.read_csv(VAL2_DATA_PATH, sep = '\\t', header = None)\n",
    "val2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "val2_df[\"label\"] = pd.read_csv(VAL2_GOLD_PATH, sep = \"\\t\", header = None)\n",
    "\n",
    "test2_df = pd.read_csv(TEST2_DATA_PATH, sep = '\\t', header = None)\n",
    "test2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "test2_df[\"label\"] = pd.read_csv(TEST2_GOLD_PATH, sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DefaultDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val_ds = DefaultDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val2_ds = DefaultDataset(\n",
    "    df=val2_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "\n",
    "# TODO: add shuffle = True\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "train_l = len(train_dl)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val_l = len(val_dl)\n",
    "val2_dl = torch.utils.data.DataLoader(val2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True)\n",
    "val2_l = len(val2_dl)\n",
    "\n",
    "train_l, val_l, val2_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ITMClassifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2name = {\n",
    "    \"acc1\": \"Accuracy@Top1\",\n",
    "    \"acc3\": \"Accuracy@Top3\",\n",
    "    \"mrr\": \"Mean Reciprocal Rank\",\n",
    "}\n",
    "\n",
    "def eval_batch(labels, preds, num_labels = NUM_PICS):\n",
    "    labels_range = np.arange(num_labels)\n",
    "    labels = labels.numpy(force=True)\n",
    "    preds = preds.numpy(force=True)\n",
    "    return {\n",
    "        \"acc1\": top_k_accuracy_score(labels, preds, k=1, labels=labels_range), \n",
    "        \"acc3\": top_k_accuracy_score(labels, preds, k=3, labels=labels_range),\n",
    "        \"mrr\": mrr(labels, preds),\n",
    "    }\n",
    "\n",
    "def sum_scores(scores, new_scores):\n",
    "    return {k: scores[k] + new_scores[k] for k in scores}\n",
    "\n",
    "def div_scores(scores, n):\n",
    "    return {k: v / n for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "num_training_steps = int(NUM_EPOCHS * (train_l / GRAD_ACCUM_STEPS))\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_STEPS_FRAC)\n",
    "# lr_scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=num_warmup_steps,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_num = 0\n",
    "steps_since_last_val = 0\n",
    "steps_since_last_val2 = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        batch = to_device(batch, DEVICE)\n",
    "        outputs = model(batch)\n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_LABELS).float().to(DEVICE))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_scores = sum_scores(train_scores, eval_batch(batch[\"label\"], outputs, num_labels = NUM_LABELS))\n",
    "\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS: \n",
    "            writer.add_scalar(\"Learning Rate\", lr_scheduler.get_last_lr()[0], step_num)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / GRAD_ACCUM_STEPS, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            steps_since_last_val += 1\n",
    "            steps_since_last_val2 += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        if steps_since_last_val == STEPS_BETWEEN_VAL:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    batch = to_device(batch, DEVICE)\n",
    "                    outputs = model(batch)\n",
    "                    loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    val_scores = sum_scores(val_scores, eval_batch(batch[\"label\"], outputs))\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss / val_l, step_num) \n",
    "            for k, v in div_scores(val_scores, val_l).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_val = 0\n",
    "        \n",
    "        if steps_since_last_val2 == STEPS_BETWEEN_VAL2:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val2_dl:\n",
    "                    batch = to_device(batch, DEVICE)\n",
    "                    outputs = model(batch)\n",
    "                    loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    val_scores = sum_scores(val_scores, eval_batch(batch[\"label\"], outputs))\n",
    "            writer.add_scalar(\"Loss/Validation 2\", val_loss / val2_l, step_num)            \n",
    "            for k, v in div_scores(val_scores, val2_l).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation 2\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_val2 = 0\n",
    "        \n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_eval(\n",
    "    model: ITMClassifier,\n",
    "    dataframes: Dict[str, pd.DataFrame],\n",
    "    images_path: Path,\n",
    "    text_processor,\n",
    "    vis_processor,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 0,\n",
    "    persistent_workers: bool = True,\n",
    "    device = torch.device(\"cpu\"),\n",
    "    preds_save_folder: Optional[Path] = None,\n",
    "    preds_save_filename_prefix: str = \"sample_predictions\",\n",
    "    preds_save_filename_add_timestamp: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Combines predictions for dataloader using checkpoint model with evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (ITMClassifier): loaded classification model\n",
    "        dataframes (pandas.DataFrame)): mapping of test set names to the dataframes\n",
    "        verbose (bool): enables prints of metrics and progress tracking\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]: predictions and scores for the corresponding test sets\n",
    "    \"\"\"\n",
    "    predictions = dict()\n",
    "    evaluations = dict()\n",
    "    for name, df in dataframes.items():\n",
    "        if verbose:\n",
    "            print(f\"Generating predictions for \\\"{name}\\\"\")\n",
    "        ds = DefaultDataset(\n",
    "            df=df,\n",
    "            images_path=images_path,\n",
    "            text_processor=text_processor,\n",
    "            vis_processor=vis_processor,\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = num_workers,\n",
    "            persistent_workers = persistent_workers,\n",
    "        )\n",
    "        preds = [] # list: \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in (tqdm(dl) if verbose else dl):\n",
    "                batch = to_device(batch, device)\n",
    "                for ps in model(batch).numpy(force=True): # ps - predictions for one row\n",
    "                    row = df.iloc[i]\n",
    "                    preds.append({row[f\"image{j}\"]: ps[j] for j in range(len(ps))})\n",
    "                    i += 1\n",
    "        predictions[name] = preds\n",
    "        if preds_save_folder is not None:\n",
    "            maybe_datetime = f\"_at_{time.time()}_\" if preds_save_filename_add_timestamp else \"_\"\n",
    "            filename = f\"{preds_save_filename_prefix}_on_{name}{maybe_datetime}submission.json\"\n",
    "            if verbose:\n",
    "                print(f\"Saving predictions for \\\"{name}\\\" as \\\"{filename}\\\"\")\n",
    "            with open(PATH / filename, \"w\") as f:\n",
    "                json.dump([{k: str(v) for k, v in p.items()} for p in preds], f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"Metrics for \\\"{name}\\\":\")\n",
    "        evals = evaluate(\n",
    "            df.iloc[:, 2:-1].values,\n",
    "            df[\"label\"].values.reshape(-1, 1),\n",
    "            preds,\n",
    "        )\n",
    "        if verbose:\n",
    "            for metric_id, metric_value in evals.items():\n",
    "                metric_name = metric2name[metric_id]\n",
    "                print(f\"    {metric_name}: {metric_value}\")\n",
    "        evaluations[name] = evals\n",
    "    return predictions, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS = [1000, 2000, 2250, 2500, 2750, 3000, 4000] # fill this out with checkpoints of interest (use Tensorboard)\n",
    "TEST_DFS = {\n",
    "    \"test\": test_df,\n",
    "    \"test2\": test2_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict() # int -> tuple(preds, evals)\n",
    "for checkpoint_num in CHECKPOINTS:\n",
    "    print(f\"Processing checkpoint {checkpoint_num}\")\n",
    "    model.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-{checkpoint_num}.pt\"))\n",
    "    results[checkpoint_num] = predict_eval(\n",
    "        model = model,\n",
    "        verbose = True,\n",
    "        preds_save_filename_prefix = f\"blip-{HEAD}-{MODEL_VERSION}-{checkpoint_num}\",\n",
    "        preds_save_folder = PATH,\n",
    "        device = DEVICE,\n",
    "        persistent_workers = PERSISTENT_WORKERS,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        batch_size = TEST_BATCH_SIZE,\n",
    "        text_processor=text_processors[\"eval\"],\n",
    "        vis_processor=vis_processors[\"eval\"],\n",
    "        dataframes = TEST_DFS,\n",
    "        images_path = IMAGES_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = {c: sum([sum(ms.values()) for ms in d[1].values()]) for c, d in results.items()}\n",
    "best_checkpoint = max(sums, key=sums.get)\n",
    "print(f\"Best checkpoint (by sum of all scores) is {best_checkpoint} with results:\")\n",
    "results[best_checkpoint][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
