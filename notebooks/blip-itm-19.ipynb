{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "import torch.nn as nn\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.utils import evaluate, mrr\n",
    "# from src.itc import ClsITC, ClsITCBatchData, Temperature\n",
    "from src.itm import AltNSDataset, to_device, ITMClassifier, DefaultDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD = \"itm\"\n",
    "MODEL_VERSION = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "VAL2_DATA_PATH = PATH / \"valid2.data.v1.txt\"\n",
    "VAL2_GOLD_PATH = PATH / \"valid2.gold.v1.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "TEST2_DATA_PATH = PATH / \"test2.data.v1.txt\"\n",
    "TEST2_GOLD_PATH = PATH / \"test2.gold.v1.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"checkpoints\").resolve() / f\"BLIP-{HEAD}-{MODEL_VERSION}\"\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "NUM_PICS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "writer = SummaryWriter(f\"runs/blip-{HEAD}-{MODEL_VERSION} (ran at {datetime.now()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "# WARNING: this is specific to my setup\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 32\n",
    "PERSISTENT_WORKERS = True\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\"\n",
    "# NUM_NS = 5\n",
    "NUM_EPOCHS = 20\n",
    "WARMUP_STEPS_FRAC = 0.1\n",
    "GRAD_ACCUM_STEPS = 30\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.1\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "# cos lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM_STEPS * TRAIN_BATCH_SIZE\n",
    "# NUM_LABELS = NUM_NS + 1\n",
    "NUM_LABELS = NUM_PICS\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_BETWEEN_VAL = 100\n",
    "STEPS_BETWEEN_VAL2 = 100\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_VAL\n",
    "VALIDATION_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = VALIDATION_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "\n",
    "val2_df = pd.read_csv(VAL2_DATA_PATH, sep = '\\t', header = None)\n",
    "val2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "val2_df[\"label\"] = pd.read_csv(VAL2_GOLD_PATH, sep = \"\\t\", header = None)\n",
    "\n",
    "test2_df = pd.read_csv(TEST2_DATA_PATH, sep = '\\t', header = None)\n",
    "test2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "test2_df[\"label\"] = pd.read_csv(TEST2_GOLD_PATH, sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2035, 86, 39)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = DefaultDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val_ds = DefaultDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val2_ds = DefaultDataset(\n",
    "    df=val2_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True, shuffle=True)\n",
    "train_l = len(train_dl)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True, shuffle=True)\n",
    "val_l = len(val_dl)\n",
    "val2_dl = torch.utils.data.DataLoader(val2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, persistent_workers=True, shuffle=True)\n",
    "val2_l = len(val2_dl)\n",
    "\n",
    "train_l, val_l, val2_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ITMClassifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2name = {\n",
    "    \"acc1\": \"Accuracy@Top1\",\n",
    "    \"acc3\": \"Accuracy@Top3\",\n",
    "    \"mrr\": \"Mean Reciprocal Rank\",\n",
    "}\n",
    "\n",
    "def eval_batch(labels, preds, num_labels = NUM_PICS):\n",
    "    labels_range = np.arange(num_labels)\n",
    "    labels = labels.numpy(force=True)\n",
    "    preds = preds.numpy(force=True)\n",
    "    return {\n",
    "        \"acc1\": top_k_accuracy_score(labels, preds, k=1, labels=labels_range), \n",
    "        \"acc3\": top_k_accuracy_score(labels, preds, k=3, labels=labels_range),\n",
    "        \"mrr\": mrr(labels, preds),\n",
    "    }\n",
    "\n",
    "def sum_scores(scores, new_scores):\n",
    "    return {k: scores[k] + new_scores[k] for k in scores}\n",
    "\n",
    "def div_scores(scores, n):\n",
    "    return {k: v / n for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1356 training steps which include 135 warmup ones\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "num_training_steps = int(NUM_EPOCHS * (train_l / GRAD_ACCUM_STEPS))\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_STEPS_FRAC)\n",
    "# lr_scheduler = get_linear_schedule_with_warmup(\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=num_warmup_steps,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1356 [02:12<9:36:10, 25.59s/it] /home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  2%|▏         | 22/1356 [09:19<9:19:41, 25.17s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  2%|▏         | 23/1356 [09:44<9:18:54, 25.16s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  4%|▍         | 51/1356 [21:30<9:08:20, 25.21s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  4%|▍         | 57/1356 [24:01<9:05:42, 25.21s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  6%|▌         | 82/1356 [34:34<8:56:00, 25.24s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  6%|▋         | 88/1356 [37:06<8:53:25, 25.24s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  7%|▋         | 100/1356 [42:08<8:48:09, 25.23s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  8%|▊         | 112/1356 [57:14<9:52:43, 28.59s/it]  /home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  8%|▊         | 115/1356 [58:29<9:02:38, 26.24s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  9%|▉         | 128/1356 [1:03:55<8:32:21, 25.03s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 10%|█         | 142/1356 [1:09:50<8:29:14, 25.17s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 11%|█         | 144/1356 [1:10:40<8:26:51, 25.09s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 11%|█         | 152/1356 [1:13:59<8:21:11, 24.98s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 12%|█▏        | 167/1356 [1:20:15<8:14:37, 24.96s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 14%|█▎        | 185/1356 [1:27:44<8:07:04, 24.96s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 15%|█▍        | 200/1356 [1:33:58<8:00:22, 24.93s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 16%|█▌        | 213/1356 [1:49:10<8:41:26, 27.37s/it]  /home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 20%|█▉        | 266/1356 [2:11:20<7:37:03, 25.16s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 21%|██        | 285/1356 [2:19:22<7:29:42, 25.19s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 22%|██▏       | 300/1356 [2:25:39<7:23:10, 25.18s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 23%|██▎       | 313/1356 [2:41:38<8:03:39, 27.82s/it]  /home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 23%|██▎       | 314/1356 [2:42:03<7:49:34, 27.04s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 24%|██▎       | 322/1356 [2:45:25<7:16:29, 25.33s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 24%|██▍       | 332/1356 [2:49:37<7:10:36, 25.23s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 25%|██▌       | 340/1356 [2:53:02<7:20:51, 26.04s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 28%|██▊       | 384/1356 [3:11:29<6:47:34, 25.16s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 29%|██▉       | 400/1356 [3:18:09<6:38:56, 25.04s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 30%|██▉       | 401/1356 [3:29:18<57:49:05, 217.95s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 31%|███▏      | 424/1356 [3:39:00<6:31:35, 25.21s/it]  "
     ]
    }
   ],
   "source": [
    "step_num = 0\n",
    "steps_since_last_val = 0\n",
    "steps_since_last_val2 = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        batch = to_device(batch, DEVICE)\n",
    "        outputs = model(batch)\n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_LABELS).float().to(DEVICE))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_scores = sum_scores(train_scores, eval_batch(batch[\"label\"], outputs, num_labels = NUM_LABELS))\n",
    "\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS: \n",
    "            writer.add_scalar(\"Learning Rate\", lr_scheduler.get_last_lr()[0], step_num)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / GRAD_ACCUM_STEPS, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            steps_since_last_val += 1\n",
    "            steps_since_last_val2 += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        if steps_since_last_val == STEPS_BETWEEN_VAL:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val_dl:\n",
    "                    batch = to_device(batch, DEVICE)\n",
    "                    outputs = model(batch)\n",
    "                    loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    val_scores = sum_scores(val_scores, eval_batch(batch[\"label\"], outputs))\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss / val_l, step_num) \n",
    "            for k, v in div_scores(val_scores, val_l).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_val = 0\n",
    "        \n",
    "        if steps_since_last_val2 == STEPS_BETWEEN_VAL2:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val2_dl:\n",
    "                    batch = to_device(batch, DEVICE)\n",
    "                    outputs = model(batch)\n",
    "                    loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    val_scores = sum_scores(val_scores, eval_batch(batch[\"label\"], outputs))\n",
    "            writer.add_scalar(\"Loss/Validation 2\", val_loss / val2_l, step_num)            \n",
    "            for k, v in div_scores(val_scores, val2_l).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation 2\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_val2 = 0\n",
    "        \n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_eval(\n",
    "    model: ITMClassifier,\n",
    "    dataframes: Dict[str, pd.DataFrame],\n",
    "    images_path: Path,\n",
    "    text_processor,\n",
    "    vis_processor,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 0,\n",
    "    persistent_workers: bool = True,\n",
    "    device = torch.device(\"cpu\"),\n",
    "    preds_save_folder: Optional[Path] = None,\n",
    "    preds_save_filename_prefix: str = \"sample_predictions\",\n",
    "    preds_save_filename_add_timestamp: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Combines predictions for dataloader using checkpoint model with evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (ITMClassifier): loaded classification model\n",
    "        dataframes (pandas.DataFrame)): mapping of test set names to the dataframes\n",
    "        verbose (bool): enables prints of metrics and progress tracking\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]: predictions and scores for the corresponding test sets\n",
    "    \"\"\"\n",
    "    predictions = dict()\n",
    "    evaluations = dict()\n",
    "    for name, df in dataframes.items():\n",
    "        if verbose:\n",
    "            print(f\"Generating predictions for \\\"{name}\\\"\")\n",
    "        ds = DefaultDataset(\n",
    "            df=df,\n",
    "            images_path=images_path,\n",
    "            text_processor=text_processor,\n",
    "            vis_processor=vis_processor,\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = num_workers,\n",
    "            persistent_workers = persistent_workers,\n",
    "        )\n",
    "        preds = [] # list: \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in (tqdm(dl) if verbose else dl):\n",
    "                batch = to_device(batch, device)\n",
    "                for ps in model(batch).numpy(force=True): # ps - predictions for one row\n",
    "                    row = df.iloc[i]\n",
    "                    preds.append({row[f\"image{j}\"]: ps[j] for j in range(len(ps))})\n",
    "                    i += 1\n",
    "        predictions[name] = preds\n",
    "        if preds_save_folder is not None:\n",
    "            maybe_datetime = f\"_at_{time.time()}_\" if preds_save_filename_add_timestamp else \"_\"\n",
    "            filename = f\"{preds_save_filename_prefix}_on_{name}{maybe_datetime}submission.json\"\n",
    "            if verbose:\n",
    "                print(f\"Saving predictions for \\\"{name}\\\" as \\\"{filename}\\\"\")\n",
    "            with open(PATH / filename, \"w\") as f:\n",
    "                json.dump([{k: str(v) for k, v in p.items()} for p in preds], f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"Metrics for \\\"{name}\\\":\")\n",
    "        evals = evaluate(\n",
    "            df.iloc[:, 2:-1].values,\n",
    "            df[\"label\"].values.reshape(-1, 1),\n",
    "            preds,\n",
    "        )\n",
    "        if verbose:\n",
    "            for metric_id, metric_value in evals.items():\n",
    "                metric_name = metric2name[metric_id]\n",
    "                print(f\"    {metric_name}: {metric_value}\")\n",
    "        evaluations[name] = evals\n",
    "    return predictions, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS = [200, 600, 700, 1000, 1200] # fill this out with checkpoints of interest (use Tensorboard)\n",
    "TEST_DFS = {\n",
    "    \"test\": test_df,\n",
    "    \"test2\": test2_df,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing checkpoint 200\n",
      "Generating predictions for \"test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [06:24<00:00,  4.58s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test\" as \"blip-itm-19-200_on_test_at_1671282958.672083_submission.json\"\n",
      "Metrics for \"test\":\n",
      "    Accuracy@Top1: 0.8224076281287247\n",
      "    Accuracy@Top3: 0.966626936829559\n",
      "    Mean Reciprocal Rank: 0.8949462701250543\n",
      "Generating predictions for \"test2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [03:15<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test2\" as \"blip-itm-19-200_on_test2_at_1671283155.266982_submission.json\"\n",
      "Metrics for \"test2\":\n",
      "    Accuracy@Top1: 0.7626683771648493\n",
      "    Accuracy@Top3: 0.9236690186016677\n",
      "    Mean Reciprocal Rank: 0.8499302564729121\n",
      "Processing checkpoint 600\n",
      "Generating predictions for \"test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [06:24<00:00,  4.57s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test\" as \"blip-itm-19-600_on_test_at_1671283540.8092375_submission.json\"\n",
      "Metrics for \"test\":\n",
      "    Accuracy@Top1: 0.8295589988081049\n",
      "    Accuracy@Top3: 0.9693087008343266\n",
      "    Mean Reciprocal Rank: 0.8998445097148912\n",
      "Generating predictions for \"test2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [03:17<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test2\" as \"blip-itm-19-600_on_test2_at_1671283739.4501984_submission.json\"\n",
      "Metrics for \"test2\":\n",
      "    Accuracy@Top1: 0.7562540089801154\n",
      "    Accuracy@Top3: 0.9236690186016677\n",
      "    Mean Reciprocal Rank: 0.8467963794455136\n",
      "Processing checkpoint 700\n",
      "Generating predictions for \"test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [06:25<00:00,  4.59s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test\" as \"blip-itm-19-700_on_test_at_1671284126.3496537_submission.json\"\n",
      "Metrics for \"test\":\n",
      "    Accuracy@Top1: 0.8253873659117997\n",
      "    Accuracy@Top3: 0.9681168057210966\n",
      "    Mean Reciprocal Rank: 0.8970350426622774\n",
      "Generating predictions for \"test2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [03:17<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test2\" as \"blip-itm-19-700_on_test2_at_1671284325.2299027_submission.json\"\n",
      "Metrics for \"test2\":\n",
      "    Accuracy@Top1: 0.7626683771648493\n",
      "    Accuracy@Top3: 0.9211032713277743\n",
      "    Mean Reciprocal Rank: 0.8509624097661301\n",
      "Processing checkpoint 1000\n",
      "Generating predictions for \"test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [06:28<00:00,  4.62s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test\" as \"blip-itm-19-1000_on_test_at_1671284714.8536747_submission.json\"\n",
      "Metrics for \"test\":\n",
      "    Accuracy@Top1: 0.8238974970202623\n",
      "    Accuracy@Top3: 0.9657330154946365\n",
      "    Mean Reciprocal Rank: 0.8957359006375694\n",
      "Generating predictions for \"test2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [03:17<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test2\" as \"blip-itm-19-1000_on_test2_at_1671284913.0068724_submission.json\"\n",
      "Metrics for \"test2\":\n",
      "    Accuracy@Top1: 0.7556125721616421\n",
      "    Accuracy@Top3: 0.9211032713277743\n",
      "    Mean Reciprocal Rank: 0.8462516672266512\n",
      "Processing checkpoint 1200\n",
      "Generating predictions for \"test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [06:27<00:00,  4.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions for \"test\" as \"blip-itm-19-1200_on_test_at_1671285302.0396414_submission.json\"\n",
      "Metrics for \"test\":\n",
      "    Accuracy@Top1: 0.8238974970202623\n",
      "    Accuracy@Top3: 0.9663289630512515\n",
      "    Mean Reciprocal Rank: 0.8959423538982537\n",
      "Generating predictions for \"test2\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 31/39 [02:45<00:42,  5.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1824808/613507478.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processing checkpoint {checkpoint_num}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_CHECKPOINT_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"step-{checkpoint_num}.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     results[checkpoint_num] = predict_eval(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1824808/2353640865.py\u001b[0m in \u001b[0;36mpredict_eval\u001b[0;34m(model, dataframes, images_path, text_processor, vis_processor, batch_size, num_workers, persistent_workers, device, preds_save_folder, preds_save_filename_prefix, preds_save_filename_add_timestamp, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# ps - predictions for one row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/vwsd/src/itm.py\u001b[0m in \u001b[0;36mto_device\u001b[0;34m(obj, device)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Implemented only for dict structures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/vwsd/src/itm.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Implemented only for dict structures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = dict() # int -> tuple(preds, evals)\n",
    "for checkpoint_num in CHECKPOINTS:\n",
    "    print(f\"Processing checkpoint {checkpoint_num}\")\n",
    "    model.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-{checkpoint_num}.pt\"))\n",
    "    results[checkpoint_num] = predict_eval(\n",
    "        model = model,\n",
    "        verbose = True,\n",
    "        preds_save_filename_prefix = f\"blip-{HEAD}-{MODEL_VERSION}-{checkpoint_num}\",\n",
    "        preds_save_folder = PATH,\n",
    "        device = DEVICE,\n",
    "        persistent_workers = PERSISTENT_WORKERS,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        batch_size = TEST_BATCH_SIZE,\n",
    "        text_processor=text_processors[\"eval\"],\n",
    "        vis_processor=vis_processors[\"eval\"],\n",
    "        dataframes = TEST_DFS,\n",
    "        images_path = IMAGES_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = {c: sum([sum(ms.values()) for ms in d[1].values()]) for c, d in results.items()}\n",
    "best_checkpoint = max(sums, key=sums.get)\n",
    "print(f\"Best checkpoint (by sum of all scores) is {best_checkpoint} with results:\")\n",
    "results[best_checkpoint][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
