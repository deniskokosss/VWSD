{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from src.data import CustomSplitLoader, ImageSet\n",
    "from src.itm import ItmDataset, to_device, ITMClassifier\n",
    "\n",
    "from src.utils import evaluate, mrr\n",
    "from src.validation import Validation, sum_scores, div_scores, eval_batch, metric2name\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD = \"itm\"\n",
    "MODEL_VERSION = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"/home/s1m00n/research/vwsd/data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "VAL2_DATA_PATH = PATH / \"valid2.data.v1.txt\"\n",
    "VAL2_GOLD_PATH = PATH / \"valid2.gold.v1.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "TEST2_DATA_PATH = PATH / \"test2.data.v1.txt\"\n",
    "TEST2_GOLD_PATH = PATH / \"test2.gold.v1.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"/home/s1m00n/research/vwsd/checkpoints\").resolve() / f\"BLIP-{HEAD}-{MODEL_VERSION}\" # TODO: maybe add timestamp?\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "NUM_PICS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "writer = SummaryWriter(f\"/home/s1m00n/research/vwsd/runs/blip-{HEAD}-{MODEL_VERSION} (ran at {datetime.now()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "# WARNING: this is specific to my setup\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 16\n",
    "PERSISTENT_WORKERS = True\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\"\n",
    "NUM_NS = 9\n",
    "NUM_EPOCHS = 40\n",
    "WARMUP_STEPS_FRAC = 0.05\n",
    "GRAD_ACCUM_STEPS = 40\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "IMAGE_AUGMENTATION = True\n",
    "# cos lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM_STEPS * TRAIN_BATCH_SIZE\n",
    "NUM_LABELS = NUM_NS + 1\n",
    "# NUM_LABELS = NUM_PICS\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_BETWEEN_VAL = 100\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_VAL\n",
    "VALIDATION_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = VALIDATION_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "\n",
    "val2_df = pd.read_csv(VAL2_DATA_PATH, sep = '\\t', header = None)\n",
    "val2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "val2_df[\"label\"] = pd.read_csv(VAL2_GOLD_PATH, sep = \"\\t\", header = None)\n",
    "\n",
    "test2_df = pd.read_csv(TEST2_DATA_PATH, sep = '\\t', header = None)\n",
    "test2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "test2_df[\"label\"] = pd.read_csv(TEST2_GOLD_PATH, sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_aug = T.AugMix()\n",
    "img_aug = T.AutoAugment()\n",
    "vis_proc = vis_processors[\"eval\"]\n",
    "vis_proc_aug = lambda p: vis_proc(img_aug(p))\n",
    "text_proc = text_processors[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_set = ImageSet(\n",
    "    images_path = IMAGES_PATH,\n",
    "    image_processor = vis_proc_aug if IMAGE_AUGMENTATION else vis_proc,\n",
    "    similarity_measure = nn.CosineSimilarity(dim=1),\n",
    "    enable_cache = False,\n",
    ")\n",
    "train_ds = ItmDataset(\n",
    "    df = train_df,\n",
    "    image_set = train_image_set,\n",
    "    text_preprocessor = text_proc,\n",
    "    use_context_as_text = True,\n",
    "    num_src_pics = NUM_PICS,\n",
    "    num_ns = NUM_NS,\n",
    "    num_any_ns = 3,\n",
    "    replace_any_ns = False,\n",
    "    replace_default_ns = False,\n",
    "    num_hard_ns = 3,\n",
    "    num_any_when_no_hard_ns = 1,\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "train_l = len(train_dl)\n",
    "train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_set = ImageSet(\n",
    "    images_path = IMAGES_PATH,\n",
    "    image_processor = vis_proc,\n",
    "    similarity_measure = nn.CosineSimilarity(dim=1),\n",
    "    enable_cache = False,\n",
    ")\n",
    "val_ds = ItmDataset(\n",
    "    df = validation_df,\n",
    "    image_set = val_image_set,\n",
    "    text_preprocessor = text_proc,\n",
    "    use_context_as_text = True,\n",
    "    num_src_pics = NUM_PICS,\n",
    "    num_ns = NUM_NS,\n",
    "    num_any_ns = 0,\n",
    "    replace_any_ns = False,\n",
    "    replace_default_ns = False,\n",
    "    num_hard_ns = 0,\n",
    "    num_any_when_no_hard_ns = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2_image_set = ImageSet(\n",
    "    images_path = IMAGES_PATH,\n",
    "    image_processor = vis_proc,\n",
    "    similarity_measure = nn.CosineSimilarity(dim=1),\n",
    "    enable_cache = False,\n",
    ")\n",
    "val2_ds = ItmDataset(\n",
    "    df = val2_df,\n",
    "    image_set = val2_image_set,\n",
    "    text_preprocessor = text_proc,\n",
    "    use_context_as_text = True,\n",
    "    num_src_pics = NUM_PICS,\n",
    "    num_ns = NUM_NS,\n",
    "    num_any_ns = 0,\n",
    "    replace_any_ns = False,\n",
    "    replace_default_ns = False,\n",
    "    num_hard_ns = 0,\n",
    "    num_any_when_no_hard_ns = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ITMClassifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "num_training_steps = int(NUM_EPOCHS * (train_l / GRAD_ACCUM_STEPS))\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_STEPS_FRAC)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_range = np.arange(NUM_PICS)\n",
    "\n",
    "def get_batch_scores(model, batch, dev, env):\n",
    "    batch = to_device(batch, dev)\n",
    "    outputs = model(batch)\n",
    "    np_labels = batch[\"label\"].numpy(force=True)\n",
    "    np_preds = outputs.numpy(force=True)\n",
    "    return {\n",
    "        \"Loss\": loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(dev)),\n",
    "        \"Accuracy@Top1\": top_k_accuracy_score(np_labels, np_preds, k=1, labels=labels_range),\n",
    "        \"Accuracy@Top3\": top_k_accuracy_score(np_labels, np_preds, k=3, labels=labels_range),\n",
    "        \"Mean Reciprocal Rank\": mrr(np_labels, np_preds),\n",
    "    }\n",
    "\n",
    "def log_score(train_step, name, metric_name, metric_value):\n",
    "    writer.add_scalar(f\"{metric_name}/{name}\", metric_value, train_step)\n",
    "    print(f\"[{train_step}][{name}]\", f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "\n",
    "validation = Validation(\n",
    "    common = {\n",
    "        \"device\": DEVICE,\n",
    "        \"get_batch_scores\": get_batch_scores,\n",
    "        \"step_cond\": lambda s: (s % STEPS_BETWEEN_VAL == 0) or (s == 1),\n",
    "        \"log_score\": log_score,\n",
    "    },\n",
    "    configs = {\n",
    "        \"Validation\": { \"dl\": torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "        # \"Validation (augmented)\": { \"dl\": torch.utils.data.DataLoader(val_aug_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "        \"Validation 2\": { \"dl\": torch.utils.data.DataLoader(val2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "        # \"Validation 2 (augmented)\": { \"dl\": torch.utils.data.DataLoader(val2_aug_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_num = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        batch = to_device(batch, DEVICE)\n",
    "        outputs = model(batch)\n",
    "        # TODO: retrieve \n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_LABELS).float().to(DEVICE))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_scores = sum_scores(train_scores, eval_batch(batch[\"label\"], outputs, num_labels = NUM_LABELS))\n",
    "\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS: \n",
    "            writer.add_scalar(\"Learning Rate\", lr_scheduler.get_last_lr()[0], step_num)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / GRAD_ACCUM_STEPS, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "            validation(step_num, model)\n",
    "\n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_eval(\n",
    "    model: ITMClassifier,\n",
    "    dataframes: Dict[str, pd.DataFrame],\n",
    "    images_path: Path,\n",
    "    text_processor,\n",
    "    vis_processors: Dict,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 0,\n",
    "    persistent_workers: bool = True,\n",
    "    device = torch.device(\"cpu\"),\n",
    "    preds_save_folder: Optional[Path] = None,\n",
    "    preds_save_filename_prefix: str = \"sample_predictions\",\n",
    "    preds_save_filename_add_timestamp: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Combines predictions for dataloader using checkpoint model with evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (ITMClassifier): loaded classification model\n",
    "        dataframes (pandas.DataFrame)): mapping of test set names to the dataframes\n",
    "        verbose (bool): enables prints of metrics and progress tracking\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]: predictions and scores for the corresponding test sets\n",
    "    \"\"\"\n",
    "    predictions = dict()\n",
    "    evaluations = dict()\n",
    "    for name, df in dataframes.items():\n",
    "        if verbose:\n",
    "            print(f\"Generating predictions for \\\"{name}\\\"\")\n",
    "        ds = DefaultDataset(\n",
    "            df=df,\n",
    "            images_path=images_path,\n",
    "            text_processor=text_processor,\n",
    "            vis_processor=vis_processors[name],\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = num_workers,\n",
    "            persistent_workers = persistent_workers,\n",
    "        )\n",
    "        preds = [] # list: \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in (tqdm(dl) if verbose else dl):\n",
    "                batch = to_device(batch, device)\n",
    "                for ps in model(batch).numpy(force=True): # ps - predictions for one row\n",
    "                    row = df.iloc[i]\n",
    "                    preds.append({row[f\"image{j}\"]: ps[j] for j in range(len(ps))})\n",
    "                    i += 1\n",
    "        predictions[name] = preds\n",
    "        if preds_save_folder is not None:\n",
    "            maybe_datetime = f\"_at_{time.time()}_\" if preds_save_filename_add_timestamp else \"_\"\n",
    "            filename = f\"{preds_save_filename_prefix}_on_{name}{maybe_datetime}submission.json\"\n",
    "            if verbose:\n",
    "                print(f\"Saving predictions for \\\"{name}\\\" as \\\"{filename}\\\"\")\n",
    "            with open(PATH / filename, \"w\") as f:\n",
    "                json.dump([{k: str(v) for k, v in p.items()} for p in preds], f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"Metrics for \\\"{name}\\\":\")\n",
    "        evals = evaluate(\n",
    "            df.iloc[:, 2:-1].values,\n",
    "            df[\"label\"].values.reshape(-1, 1),\n",
    "            preds,\n",
    "        )\n",
    "        if verbose:\n",
    "            for metric_id, metric_value in evals.items():\n",
    "                metric_name = metric2name[metric_id]\n",
    "                print(f\"    {metric_name}: {metric_value}\")\n",
    "        evaluations[name] = evals\n",
    "    return predictions, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS = [500, 1000, 1300, 2000] # fill this out with checkpoints of interest (use Tensorboard)\n",
    "TEST_DFS = {\n",
    "    \"Test\": test_df,\n",
    "    \"Test 2\": test2_df,\n",
    "    \"Test (augmented)\": test_df,\n",
    "    \"Test 2 (augmented)\": test2_df,\n",
    "}\n",
    "TEST_VIS_PROCS = {\n",
    "    \"Test\": vis_proc,\n",
    "    \"Test 2\": vis_proc,\n",
    "    \"Test (augmented)\": vis_proc_aug,\n",
    "    \"Test 2 (augmented)\": vis_proc_aug,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict() # int -> tuple(preds, evals)\n",
    "for checkpoint_num in CHECKPOINTS:\n",
    "    print(f\"Processing checkpoint {checkpoint_num}\")\n",
    "    model.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-{checkpoint_num}.pt\"))\n",
    "    results[checkpoint_num] = predict_eval(\n",
    "        model = model,\n",
    "        verbose = True,\n",
    "        preds_save_filename_prefix = f\"blip-{HEAD}-{MODEL_VERSION}-{checkpoint_num}\",\n",
    "        preds_save_folder = PATH,\n",
    "        device = DEVICE,\n",
    "        persistent_workers = PERSISTENT_WORKERS,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        batch_size = TEST_BATCH_SIZE,\n",
    "        text_processor=text_processors[\"eval\"],\n",
    "        vis_processors=TEST_VIS_PROCS,\n",
    "        dataframes = TEST_DFS,\n",
    "        images_path = IMAGES_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = {c: sum([sum(ms.values()) for ms in d[1].values()]) for c, d in results.items()}\n",
    "best_checkpoint = max(sums, key=sums.get)\n",
    "print(f\"Best checkpoint (by sum of all scores) is {best_checkpoint} with results:\")\n",
    "results[best_checkpoint][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "lavis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
