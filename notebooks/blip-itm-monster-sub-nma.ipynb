{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.itm import AltNSDataset, to_device, ITMClassifier, DefaultDataset, DefaultDatasetMultiaug\n",
    "\n",
    "from src.utils import evaluate, mrr\n",
    "from src.validation import Validation, sum_scores, div_scores, eval_batch, metric2name\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD = \"itm\"\n",
    "MODEL_VERSION = \"monster-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"/home/s1m00n/research/vwsd/data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "VAL2_DATA_PATH = PATH / \"valid2.data.v1.txt\"\n",
    "VAL2_GOLD_PATH = PATH / \"valid2.gold.v1.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "TEST2_DATA_PATH = PATH / \"test2.data.v1.txt\"\n",
    "TEST2_GOLD_PATH = PATH / \"test2.gold.v1.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"/home/s1m00n/research/vwsd/checkpoints\").resolve() / f\"BLIP-{HEAD}-{MODEL_VERSION}\" # TODO: maybe add timestamp?\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "NUM_PICS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "# WARNING: this is specific to my setup\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_WORKERS = 0\n",
    "PERSISTENT_WORKERS = True\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLIP_VARIANT = \"large\" # \"base\" | \"large\"\n",
    "NUM_NS = 9\n",
    "NUM_EPOCHS = 15\n",
    "WARMUP_STEPS_FRAC = 0.15\n",
    "GRAD_ACCUM_STEPS = 16\n",
    "LR = 5e-6\n",
    "WEIGHT_DECAY = 1e-4\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "IMAGE_AUGMENTATION = True\n",
    "VALIDATE_UNAUGMENTED = True\n",
    "VALIDATE_AUGMENTED = True\n",
    "# cos lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM_STEPS * TRAIN_BATCH_SIZE\n",
    "NUM_LABELS = NUM_NS + 1\n",
    "# NUM_LABELS = NUM_PICS\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS_BETWEEN_VAL = 1000\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_VAL\n",
    "VALIDATION_BATCH_SIZE = 20\n",
    "TEST_BATCH_SIZE = VALIDATION_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>context</th>\n",
       "      <th>image0</th>\n",
       "      <th>image1</th>\n",
       "      <th>image2</th>\n",
       "      <th>image3</th>\n",
       "      <th>image4</th>\n",
       "      <th>image5</th>\n",
       "      <th>image6</th>\n",
       "      <th>image7</th>\n",
       "      <th>image8</th>\n",
       "      <th>image9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goal</td>\n",
       "      <td>football goal</td>\n",
       "      <td>image.4418.jpg</td>\n",
       "      <td>image.4416.jpg</td>\n",
       "      <td>image.4417.jpg</td>\n",
       "      <td>image.4413.jpg</td>\n",
       "      <td>image.4412.jpg</td>\n",
       "      <td>image.4415.jpg</td>\n",
       "      <td>image.4419.jpg</td>\n",
       "      <td>image.4414.jpg</td>\n",
       "      <td>image.2166.jpg</td>\n",
       "      <td>image.1150.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mustard</td>\n",
       "      <td>mustard seed</td>\n",
       "      <td>image.4429.png</td>\n",
       "      <td>image.4422.jpg</td>\n",
       "      <td>image.4423.jpg</td>\n",
       "      <td>image.4424.jpg</td>\n",
       "      <td>image.4421.jpg</td>\n",
       "      <td>image.4427.jpg</td>\n",
       "      <td>image.4426.jpg</td>\n",
       "      <td>image.4420.jpg</td>\n",
       "      <td>image.4425.jpg</td>\n",
       "      <td>image.4428.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seat</td>\n",
       "      <td>eating seat</td>\n",
       "      <td>image.4435.jpg</td>\n",
       "      <td>image.4436.jpg</td>\n",
       "      <td>image.1166.jpg</td>\n",
       "      <td>image.4430.jpg</td>\n",
       "      <td>image.4433.jpg</td>\n",
       "      <td>image.4432.jpg</td>\n",
       "      <td>image.4438.jpg</td>\n",
       "      <td>image.4434.jpg</td>\n",
       "      <td>image.4431.jpg</td>\n",
       "      <td>image.4437.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>navigate</td>\n",
       "      <td>navigate the web</td>\n",
       "      <td>image.4439.jpg</td>\n",
       "      <td>image.4440.jpg</td>\n",
       "      <td>image.4441.jpg</td>\n",
       "      <td>image.4442.jpg</td>\n",
       "      <td>image.4444.jpg</td>\n",
       "      <td>image.4445.jpg</td>\n",
       "      <td>image.1435.jpg</td>\n",
       "      <td>image.4446.png</td>\n",
       "      <td>image.1434.jpg</td>\n",
       "      <td>image.4443.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>butterball</td>\n",
       "      <td>butterball person</td>\n",
       "      <td>image.4454.jpg</td>\n",
       "      <td>image.4450.jpg</td>\n",
       "      <td>image.4455.jpg</td>\n",
       "      <td>image.4453.jpg</td>\n",
       "      <td>image.4448.jpg</td>\n",
       "      <td>image.1253.jpg</td>\n",
       "      <td>image.4451.jpg</td>\n",
       "      <td>image.4452.jpg</td>\n",
       "      <td>image.4447.jpg</td>\n",
       "      <td>image.4449.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>cannabis</td>\n",
       "      <td>cannabis drug</td>\n",
       "      <td>image.8063.jpg</td>\n",
       "      <td>image.8064.jpg</td>\n",
       "      <td>image.4891.jpg</td>\n",
       "      <td>image.7450.jpg</td>\n",
       "      <td>image.8066.jpg</td>\n",
       "      <td>image.4454.jpg</td>\n",
       "      <td>image.8065.jpg</td>\n",
       "      <td>image.6775.jpg</td>\n",
       "      <td>image.1604.jpg</td>\n",
       "      <td>image.2540.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>crossroads</td>\n",
       "      <td>crossroads cars</td>\n",
       "      <td>image.8073.jpg</td>\n",
       "      <td>image.8076.jpg</td>\n",
       "      <td>image.8075.jpg</td>\n",
       "      <td>image.8070.jpg</td>\n",
       "      <td>image.8068.jpg</td>\n",
       "      <td>image.8074.jpg</td>\n",
       "      <td>image.8069.jpg</td>\n",
       "      <td>image.8071.jpg</td>\n",
       "      <td>image.8067.jpg</td>\n",
       "      <td>image.8072.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>clocks</td>\n",
       "      <td>time clocks</td>\n",
       "      <td>image.8082.jpg</td>\n",
       "      <td>image.8079.jpg</td>\n",
       "      <td>image.2094.jpg</td>\n",
       "      <td>image.8081.jpg</td>\n",
       "      <td>image.8077.jpg</td>\n",
       "      <td>image.8080.jpg</td>\n",
       "      <td>image.4995.jpg</td>\n",
       "      <td>image.8083.jpg</td>\n",
       "      <td>image.5251.jpg</td>\n",
       "      <td>image.8078.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>columba</td>\n",
       "      <td>columba stars</td>\n",
       "      <td>image.8087.jpg</td>\n",
       "      <td>image.8084.jpg</td>\n",
       "      <td>image.7279.jpg</td>\n",
       "      <td>image.192.jpg</td>\n",
       "      <td>image.93.jpg</td>\n",
       "      <td>image.8085.jpg</td>\n",
       "      <td>image.8088.jpg</td>\n",
       "      <td>image.4126.jpg</td>\n",
       "      <td>image.8086.jpg</td>\n",
       "      <td>image.8089.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>diamond</td>\n",
       "      <td>diamond field</td>\n",
       "      <td>image.8090.jpg</td>\n",
       "      <td>image.8091.jpg</td>\n",
       "      <td>image.8092.jpg</td>\n",
       "      <td>image.8093.jpg</td>\n",
       "      <td>image.8098.jpg</td>\n",
       "      <td>image.8097.jpg</td>\n",
       "      <td>image.8094.jpg</td>\n",
       "      <td>image.8099.jpg</td>\n",
       "      <td>image.8095.jpg</td>\n",
       "      <td>image.8096.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word            context          image0          image1  \\\n",
       "0          goal      football goal  image.4418.jpg  image.4416.jpg   \n",
       "1       mustard       mustard seed  image.4429.png  image.4422.jpg   \n",
       "2          seat        eating seat  image.4435.jpg  image.4436.jpg   \n",
       "3      navigate   navigate the web  image.4439.jpg  image.4440.jpg   \n",
       "4    butterball  butterball person  image.4454.jpg  image.4450.jpg   \n",
       "..          ...                ...             ...             ...   \n",
       "458    cannabis      cannabis drug  image.8063.jpg  image.8064.jpg   \n",
       "459  crossroads    crossroads cars  image.8073.jpg  image.8076.jpg   \n",
       "460      clocks        time clocks  image.8082.jpg  image.8079.jpg   \n",
       "461     columba      columba stars  image.8087.jpg  image.8084.jpg   \n",
       "462     diamond      diamond field  image.8090.jpg  image.8091.jpg   \n",
       "\n",
       "             image2          image3          image4          image5  \\\n",
       "0    image.4417.jpg  image.4413.jpg  image.4412.jpg  image.4415.jpg   \n",
       "1    image.4423.jpg  image.4424.jpg  image.4421.jpg  image.4427.jpg   \n",
       "2    image.1166.jpg  image.4430.jpg  image.4433.jpg  image.4432.jpg   \n",
       "3    image.4441.jpg  image.4442.jpg  image.4444.jpg  image.4445.jpg   \n",
       "4    image.4455.jpg  image.4453.jpg  image.4448.jpg  image.1253.jpg   \n",
       "..              ...             ...             ...             ...   \n",
       "458  image.4891.jpg  image.7450.jpg  image.8066.jpg  image.4454.jpg   \n",
       "459  image.8075.jpg  image.8070.jpg  image.8068.jpg  image.8074.jpg   \n",
       "460  image.2094.jpg  image.8081.jpg  image.8077.jpg  image.8080.jpg   \n",
       "461  image.7279.jpg   image.192.jpg    image.93.jpg  image.8085.jpg   \n",
       "462  image.8092.jpg  image.8093.jpg  image.8098.jpg  image.8097.jpg   \n",
       "\n",
       "             image6          image7          image8          image9  \n",
       "0    image.4419.jpg  image.4414.jpg  image.2166.jpg  image.1150.jpg  \n",
       "1    image.4426.jpg  image.4420.jpg  image.4425.jpg  image.4428.jpg  \n",
       "2    image.4438.jpg  image.4434.jpg  image.4431.jpg  image.4437.jpg  \n",
       "3    image.1435.jpg  image.4446.png  image.1434.jpg  image.4443.jpg  \n",
       "4    image.4451.jpg  image.4452.jpg  image.4447.jpg  image.4449.jpg  \n",
       "..              ...             ...             ...             ...  \n",
       "458  image.8065.jpg  image.6775.jpg  image.1604.jpg  image.2540.jpg  \n",
       "459  image.8069.jpg  image.8071.jpg  image.8067.jpg  image.8072.jpg  \n",
       "460  image.4995.jpg  image.8083.jpg  image.5251.jpg  image.8078.jpg  \n",
       "461  image.8088.jpg  image.4126.jpg  image.8086.jpg  image.8089.jpg  \n",
       "462  image.8094.jpg  image.8099.jpg  image.8095.jpg  image.8096.jpg  \n",
       "\n",
       "[463 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df = pd.read_csv(\"/home/s1m00n/research/vwsd/data/test.data.v1.1/en.test.data.v1.1.txt\", sep=\"\\t\", header=None)\n",
    "en_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "en_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)\n",
    "img_aug = T.TrivialAugmentWide()\n",
    "vis_proc = vis_processors[\"eval\"]\n",
    "vis_proc_aug = lambda p: vis_proc(img_aug(p))\n",
    "test_ds = DefaultDataset(\n",
    "    df = en_df,\n",
    "    images_path=Path(\"/home/s1m00n/research/vwsd/data/test_images/test_images\").resolve(),\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_proc,\n",
    "    ignore_labels=True,\n",
    ")\n",
    "\n",
    "test_aug_ds = DefaultDatasetMultiaug(\n",
    "    df = en_df,\n",
    "    images_path=Path(\"/home/s1m00n/research/vwsd/data/test_images/test_images\").resolve(),\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_proc,\n",
    "    vis_aug = img_aug,\n",
    "    n_aug = 9,\n",
    "    include_original = True,\n",
    "    ignore_labels=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ITMClassifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-monster-v1/step-12000.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiaug_extract(\n",
    "    imgs: torch.Tensor # (bs, 1 + n_aug, num_pics, ...)\n",
    ") -> torch.Tensor: # [(bs, num_pics, ...)] with len = 1 + n_aug\n",
    "    return [imgs[:, i] for i in range(imgs.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size = 20,\n",
    "    shuffle = False,\n",
    "    num_workers = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_dl = torch.utils.data.DataLoader(\n",
    "    test_aug_ds,\n",
    "    batch_size = 20,\n",
    "    shuffle = False,\n",
    "    num_workers = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 3/24 [05:25<31:49, 90.92s/it]   ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      " 17%|█▋        | 4/24 [06:55<34:36, 103.84s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 286575) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_286293/1546776159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"images\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp_preds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mnp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lavis/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/vwsd/src/itm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblip_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text_input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"itm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_negative_probas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;31m# TODO: optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lavis/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/lavis/lib/python3.9/site-packages/lavis/models/blip_models/blip_image_text_matching.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, samples, match_head)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mimage_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisual_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/lavis/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 286575) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit."
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for batch in tqdm(aug_dl, total=len(aug_dl)):\n",
    "        imgs = multiaug_extract(batch[\"images\"])\n",
    "        np_preds = None\n",
    "        for bi in imgs:\n",
    "            batch[\"images\"] = bi\n",
    "            out = model(to_device(batch, DEVICE))\n",
    "            if np_preds is None:\n",
    "                np_preds = out.numpy(force=True)\n",
    "            else:\n",
    "                np_preds += out.numpy(force=True)\n",
    "        np_preds = np_preds / len(imgs)\n",
    "        for ps in np.argsort(-np_preds, axis=1):\n",
    "            row = en_df.iloc[i]\n",
    "            preds.append([row[f\"image{j}\"] for j in ps])\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"blip-itm-monster-v1-real-test-9aug.en.txt\", \"w\") as f:\n",
    "    for row in preds:\n",
    "        f.write(\"\\t\".join(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_range = np.arange(NUM_PICS)\n",
    "\n",
    "def get_batch_scores(model, batch, dev, env):\n",
    "    batch = to_device(batch, dev)\n",
    "    outputs = model(batch)\n",
    "    np_labels = batch[\"label\"].numpy(force=True)\n",
    "    np_preds = outputs.numpy(force=True)\n",
    "    return {\n",
    "        \"Loss\": loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_PICS).float().to(dev)),\n",
    "        \"Accuracy@Top1\": top_k_accuracy_score(np_labels, np_preds, k=1, labels=labels_range),\n",
    "        \"Accuracy@Top3\": top_k_accuracy_score(np_labels, np_preds, k=3, labels=labels_range),\n",
    "        \"Mean Reciprocal Rank\": mrr(np_labels, np_preds),\n",
    "    }\n",
    "\n",
    "def get_batch_scores_aug(model, batch, dev, env):\n",
    "    np_labels = batch[\"label\"].numpy(force=True)\n",
    "    imgs = multiaug_extract(batch[\"images\"])\n",
    "    np_preds = None\n",
    "    loss_accum = 0\n",
    "    for bi in imgs:\n",
    "        batch[\"images\"] = bi\n",
    "        out = model(to_device(batch, dev))\n",
    "        if np_preds is None:\n",
    "            np_preds = out.numpy(force=True)\n",
    "        else:\n",
    "            np_preds += out.numpy(force=True)\n",
    "        loss_accum += loss_fn(out, F.one_hot(batch[\"label\"], NUM_PICS).float().to(dev)).item()\n",
    "    np_preds = np_preds / len(imgs)\n",
    "    return {\n",
    "        \"Loss\": loss_accum / len(imgs),\n",
    "        \"Accuracy@Top1\": top_k_accuracy_score(np_labels, np_preds, k=1, labels=labels_range),\n",
    "        \"Accuracy@Top3\": top_k_accuracy_score(np_labels, np_preds, k=3, labels=labels_range),\n",
    "        \"Mean Reciprocal Rank\": mrr(np_labels, np_preds),\n",
    "    }\n",
    "\n",
    "def log_score(train_step, name, metric_name, metric_value):\n",
    "    writer.add_scalar(f\"{metric_name}/{name}\", metric_value, train_step)\n",
    "    # print(f\"[{train_step}][{name}]\", f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "\n",
    "validation = Validation(\n",
    "    common = {\n",
    "        \"device\": DEVICE,\n",
    "        \"get_batch_scores\": get_batch_scores_aug,\n",
    "        \"step_cond\": lambda s: (s % STEPS_BETWEEN_VAL == 0) or (s == 1),\n",
    "        \"log_score\": log_score,\n",
    "    },\n",
    "    configs = {\n",
    "        \"Validation\": {\n",
    "            \"dl\": torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True),\n",
    "        },\n",
    "        \"Validation 2\": {\n",
    "            \"dl\": torch.utils.data.DataLoader(val2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True),\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_num = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        batch = to_device(batch, DEVICE)\n",
    "        outputs = model(batch)\n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_LABELS).float().to(DEVICE))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_scores = sum_scores(train_scores, eval_batch(batch[\"label\"], outputs, num_labels = NUM_LABELS))\n",
    "\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS: \n",
    "            writer.add_scalar(\"Learning Rate\", lr_scheduler.get_last_lr()[0], step_num)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / GRAD_ACCUM_STEPS, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "            validation(step_num, model)\n",
    "\n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_eval(\n",
    "    model: ITMClassifier,\n",
    "    dataframes: Dict[str, pd.DataFrame],\n",
    "    images_path: Path,\n",
    "    text_processor,\n",
    "    vis_processors: Dict,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 0,\n",
    "    persistent_workers: bool = True,\n",
    "    device = torch.device(\"cpu\"),\n",
    "    preds_save_folder: Optional[Path] = None,\n",
    "    preds_save_filename_prefix: str = \"sample_predictions\",\n",
    "    preds_save_filename_add_timestamp: bool = True,\n",
    "    verbose: bool = True,\n",
    ") -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Combines predictions for dataloader using checkpoint model with evaluation.\n",
    "\n",
    "    Args:\n",
    "        model (ITMClassifier): loaded classification model\n",
    "        dataframes (pandas.DataFrame)): mapping of test set names to the dataframes\n",
    "        verbose (bool): enables prints of metrics and progress tracking\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, Dict[str, float]]]: predictions and scores for the corresponding test sets\n",
    "    \"\"\"\n",
    "    predictions = dict()\n",
    "    evaluations = dict()\n",
    "    for name, df in dataframes.items():\n",
    "        if verbose:\n",
    "            print(f\"Generating predictions for \\\"{name}\\\"\")\n",
    "        ds = DefaultDataset(\n",
    "            df=df,\n",
    "            images_path=images_path,\n",
    "            text_processor=text_processor,\n",
    "            vis_processor=vis_processors[name],\n",
    "        )\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            ds,\n",
    "            batch_size = batch_size,\n",
    "            shuffle = False,\n",
    "            num_workers = num_workers,\n",
    "            persistent_workers = persistent_workers,\n",
    "        )\n",
    "        preds = [] # list: \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        i = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in (tqdm(dl) if verbose else dl):\n",
    "                batch = to_device(batch, device)\n",
    "                for ps in model(batch).numpy(force=True): # ps - predictions for one row\n",
    "                    row = df.iloc[i]\n",
    "                    preds.append({row[f\"image{j}\"]: ps[j] for j in range(len(ps))})\n",
    "                    i += 1\n",
    "        predictions[name] = preds\n",
    "        if preds_save_folder is not None:\n",
    "            maybe_datetime = f\"_at_{time.time()}_\" if preds_save_filename_add_timestamp else \"_\"\n",
    "            filename = f\"{preds_save_filename_prefix}_on_{name}{maybe_datetime}submission.json\"\n",
    "            if verbose:\n",
    "                print(f\"Saving predictions for \\\"{name}\\\" as \\\"{filename}\\\"\")\n",
    "            with open(PATH / filename, \"w\") as f:\n",
    "                json.dump([{k: str(v) for k, v in p.items()} for p in preds], f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"Metrics for \\\"{name}\\\":\")\n",
    "        evals = evaluate(\n",
    "            df.iloc[:, 2:-1].values,\n",
    "            df[\"label\"].values.reshape(-1, 1),\n",
    "            preds,\n",
    "        )\n",
    "        if verbose:\n",
    "            for metric_id, metric_value in evals.items():\n",
    "                metric_name = metric2name[metric_id]\n",
    "                print(f\"    {metric_name}: {metric_value}\")\n",
    "        evaluations[name] = evals\n",
    "    return predictions, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINTS = [500, 1000, 1300, 2000] # fill this out with checkpoints of interest (use Tensorboard)\n",
    "TEST_DFS = {\n",
    "    \"Test\": test_df,\n",
    "    \"Test 2\": test2_df,\n",
    "    \"Test (augmented)\": test_df,\n",
    "    \"Test 2 (augmented)\": test2_df,\n",
    "}\n",
    "TEST_VIS_PROCS = {\n",
    "    \"Test\": vis_proc,\n",
    "    \"Test 2\": vis_proc,\n",
    "    \"Test (augmented)\": vis_proc_aug,\n",
    "    \"Test 2 (augmented)\": vis_proc_aug,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict() # int -> tuple(preds, evals)\n",
    "for checkpoint_num in CHECKPOINTS:\n",
    "    print(f\"Processing checkpoint {checkpoint_num}\")\n",
    "    model.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-{checkpoint_num}.pt\"))\n",
    "    results[checkpoint_num] = predict_eval(\n",
    "        model = model,\n",
    "        verbose = True,\n",
    "        preds_save_filename_prefix = f\"blip-{HEAD}-{MODEL_VERSION}-{checkpoint_num}\",\n",
    "        preds_save_folder = PATH,\n",
    "        device = DEVICE,\n",
    "        persistent_workers = PERSISTENT_WORKERS,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        batch_size = TEST_BATCH_SIZE,\n",
    "        text_processor=text_processors[\"eval\"],\n",
    "        vis_processors=TEST_VIS_PROCS,\n",
    "        dataframes = TEST_DFS,\n",
    "        images_path = IMAGES_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = {c: sum([sum(ms.values()) for ms in d[1].values()]) for c, d in results.items()}\n",
    "best_checkpoint = max(sums, key=sums.get)\n",
    "print(f\"Best checkpoint (by sum of all scores) is {best_checkpoint} with results:\")\n",
    "results[best_checkpoint][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "lavis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
