{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP ITM finetuning to target task\n",
    "\n",
    "Batch is formed from a single row of dataset:\n",
    "$$\\operatorname{batch} = ((E_t, E_{i_0}), (E_t, E_{i_1}), ..., (E_t, E_{i_9})); \\operatorname{batch} : R^{10 \\times (E_t + E_i)}$$\n",
    "ITM predicts probas for $y = 0$, $y = 1$\n",
    "$$\\operatorname{ITM} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10 \\times 2}$$\n",
    "Model is defined as:\n",
    "$$\\operatorname{F} = \\operatorname{softmax} \\circ \\operatorname{ITM} \\circ \\operatorname{batch}$$\n",
    "$$\\operatorname{F} : R^{10 \\times (E_t + E_i)} \\rightarrow R^{10}$$\n",
    "So, this definition is for a single row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageFile\n",
    "import torch.nn as nn\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchmetrics.functional import retrieval_reciprocal_rank, retrieval_hit_rate\n",
    "from transformers import BatchEncoding\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.utils import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "SAVE_CHECKPOINT_PATH = Path(\"checkpoints\").resolve() / \"BLIP-ITM-2\"\n",
    "SAVE_CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\"\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_STEPS_FRAC = 0.1\n",
    "STEPS_BETWEEN_EVAL = 25\n",
    "GRAD_ACCUM_STEPS = 32\n",
    "SAVE_CHECKPOINT_STEPS = STEPS_BETWEEN_EVAL \n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_repeat(value):\n",
    "    while True:\n",
    "        yield value\n",
    "\n",
    "def concat_iters(*iterables):\n",
    "    for it in iterables:\n",
    "        for value in it:\n",
    "            yield value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        images_path: Path,\n",
    "        text_processor,\n",
    "        vis_processor,\n",
    "        use_context_as_text: bool = True,\n",
    "        enable_cache: bool = True,\n",
    "    ) -> None:\n",
    "        self.df = df\n",
    "        self.images_path = images_path\n",
    "        self.text_processor = text_processor\n",
    "        self.vis_processor = vis_processor\n",
    "        self.tokens_cache = dict()\n",
    "        self.image_batch_cache = dict()\n",
    "        self.enable_cache = enable_cache\n",
    "        self.text_field = \"context\" if use_context_as_text else \"word\"\n",
    "        self.labels_map = self._gen_labels()\n",
    "\n",
    "    def _gen_labels(self) -> Dict[int, int]: # index to label\n",
    "        labels = self.df[\"label\"].values\n",
    "        zips = []\n",
    "        for i in range(10):\n",
    "            images = self.df[f\"image{i}\"].values\n",
    "            zips.append(zip(np.argwhere(labels == images).reshape(-1), infinite_repeat(i)))\n",
    "        return dict(concat_iters(*tuple(zips)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _get_image(self, name: str) -> Image:\n",
    "        return Image.open(self.images_path / name).convert(\"RGB\")\n",
    "\n",
    "    def _make_image_batch(self, idx: int) -> torch.Tensor:\n",
    "        row = self.df.iloc[idx]\n",
    "        return torch.stack([self.vis_processor(self._get_image(row[f\"image{i}\"])) for i in range(10)])\n",
    "\n",
    "    def _get_image_batch(self, idx: int) -> torch.Tensor:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_image_batch(idx)\n",
    "        if idx in self.image_batch_cache:\n",
    "            return self.image_batch_cache[idx]\n",
    "        t = self._make_image_batch(idx)\n",
    "        self.image_batch_cache[idx] = t\n",
    "        return t\n",
    "    \n",
    "    def _make_tokens(self, idx: int) -> BatchEncoding:\n",
    "        return self.text_processor(self.df.iloc[idx][self.text_field])\n",
    "    \n",
    "    def _get_tokens(self, idx: int) -> BatchEncoding:\n",
    "        if not self.enable_cache:\n",
    "            return self._make_tokens(idx)\n",
    "        if idx in self.tokens_cache:\n",
    "            return self.tokens_cache[idx]\n",
    "        t = self._make_tokens(idx)\n",
    "        self.tokens_cache[idx] = t\n",
    "        return t\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, Union[torch.Tensor, BatchEncoding, int]]:\n",
    "        # makes a batch for each row!\n",
    "        return {\n",
    "            \"text\": self._get_tokens(idx),\n",
    "            \"images\": self._get_image_batch(idx),\n",
    "            \"label\": self.labels_map[idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ItmDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    "    enable_cache=False # eats up too much ram, whole 128GB!\n",
    ")\n",
    "val_ds = ItmDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    "    # here enable_cache is left as is (True), because we want fast & frequent validations\n",
    ")\n",
    "test_ds = ItmDataset(\n",
    "    df=test_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    "    enable_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(object, device):\n",
    "    if not isinstance(object, dict):\n",
    "        raise NotImplementedError(\"Implement other types than dict if needed!\")\n",
    "    return {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in object.items()}\n",
    "\n",
    "def label2bool_tensor(label: int) -> torch.Tensor:\n",
    "    t = torch.zeros(10, dtype=torch.bool)\n",
    "    t[label] = True\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, blip_model: BlipBase) -> None:\n",
    "        super().__init__()\n",
    "        self.blip_model = blip_model\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        text_feats = inputs[\"text\"]\n",
    "        images_feats = inputs[\"images\"]\n",
    "        batch_outputs = self.blip_model({\"text_input\": [text_feats for _ in range(10)], \"image\": images_feats}, match_head=\"itm\")\n",
    "        return F.softmax(batch_outputs[:, 1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric2name = {\n",
    "    \"acc1\": \"Accuracy@Top1\",\n",
    "    \"acc3\": \"Accuracy@Top3\",\n",
    "    \"mrr\": \"Mean Reciprocal Rank\",\n",
    "}\n",
    "\n",
    "def eval_single(model_outputs, one_hot_label_tensor):\n",
    "    return {\n",
    "        \"acc1\": retrieval_hit_rate(model_outputs, one_hot_label_tensor, 1).item(),\n",
    "        \"acc3\": retrieval_hit_rate(model_outputs, one_hot_label_tensor, 3).item(),\n",
    "        \"mrr\": retrieval_reciprocal_rank(model_outputs, one_hot_label_tensor).item(),\n",
    "    }\n",
    "\n",
    "def sum_scores(scores, new_scores):\n",
    "    return {k: scores[k] + new_scores[k] for k in scores}\n",
    "\n",
    "def div_scores(scores, n):\n",
    "    return {k: v / n for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 training steps which include 57 warmup ones\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = int(NUM_EPOCHS * len(train_ds) / GRAD_ACCUM_STEPS)\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_STEPS_FRAC)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 25/572 [11:50<4:10:03, 27.43s/it]/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "  9%|▊         | 50/572 [1:08:05<4:08:54, 28.61s/it] INFO:root:[0:50] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-50.pt\"\n",
      " 17%|█▋        | 100/572 [1:58:06<3:44:47, 28.57s/it] INFO:root:[0:100] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-100.pt\"\n",
      " 26%|██▌       | 150/572 [2:48:19<3:18:15, 28.19s/it]  INFO:root:[0:150] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-150.pt\"\n",
      " 35%|███▍      | 200/572 [3:39:05<2:57:04, 28.56s/it]  INFO:root:[1:200] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-200.pt\"\n",
      " 44%|████▎     | 250/572 [4:29:29<2:30:37, 28.07s/it]  INFO:root:[1:250] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-250.pt\"\n",
      " 52%|█████▏    | 300/572 [5:19:32<2:05:54, 27.77s/it]  INFO:root:[1:300] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-300.pt\"\n",
      " 61%|██████    | 350/572 [6:10:08<1:51:46, 30.21s/it]  INFO:root:[1:350] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-350.pt\"\n",
      " 70%|██████▉   | 400/572 [7:00:49<1:25:04, 29.68s/it]  INFO:root:[2:400] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-400.pt\"\n",
      " 79%|███████▊  | 450/572 [7:51:09<56:50, 27.96s/it]    INFO:root:[2:450] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-450.pt\"\n",
      " 87%|████████▋ | 500/572 [8:41:29<36:35, 30.49s/it]   INFO:root:[2:500] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-500.pt\"\n",
      " 96%|█████████▌| 550/572 [9:32:02<10:32, 28.74s/it]   INFO:root:[2:550] Saved checkpoint at \"/home/s1m00n/research/vwsd/checkpoints/BLIP-ITM-2/step-550.pt\"\n",
      " 98%|█████████▊| 559/572 [9:49:45<09:14, 42.63s/it]   "
     ]
    }
   ],
   "source": [
    "step_num = 0\n",
    "steps_since_last_eval = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_ds:\n",
    "        outputs = model(to_device(batch, DEVICE))\n",
    "        loss = loss_fn(outputs, torch.tensor(batch[\"label\"]).to(DEVICE))\n",
    "        train_loss += loss.item()\n",
    "        new_scores = eval_single(outputs, label2bool_tensor(batch[\"label\"]).to(DEVICE))\n",
    "        train_scores = sum_scores(train_scores, new_scores)\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM_STEPS:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            writer.add_scalar(\"Loss/Train\", float(train_loss / GRAD_ACCUM_STEPS), step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM_STEPS).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            steps_since_last_eval += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        if steps_since_last_eval == STEPS_BETWEEN_EVAL:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            with torch.no_grad():\n",
    "                for batch in val_ds:\n",
    "                    outputs = model(to_device(batch, DEVICE))\n",
    "                    loss = loss_fn(outputs, torch.tensor(batch[\"label\"]).to(DEVICE))\n",
    "                    val_loss += loss.item()\n",
    "                    new_scores = eval_single(outputs, label2bool_tensor(batch[\"label\"]).to(DEVICE))\n",
    "                    val_scores = sum_scores(val_scores, new_scores)\n",
    "            writer.add_scalar(\"Loss/Validation\", val_loss / len(val_ds), step_num)            \n",
    "            for k, v in div_scores(val_scores, len(val_ds)).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Validation\", v, step_num)\n",
    "            model.train()\n",
    "            steps_since_last_eval = 0\n",
    "        \n",
    "        if save_checkpoint_step_cnt == SAVE_CHECKPOINT_STEPS:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = SAVE_CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            logging.info(f\"[{epoch_num}:{step_num}] Saving checkpoint to \\\"{str(p)}\\\"\")\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let's load the best checkpoint according to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Classifier(blip_model).to(DEVICE)\n",
    "checkpoint.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-400.pt\"))\n",
    "checkpoint.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 3356/3356 [42:21<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (i, batch) in enumerate(tqdm(test_ds)):\n",
    "        preds = checkpoint(to_device(batch, DEVICE)).numpy(force=True)\n",
    "        row = test_df.iloc[i]\n",
    "        predictions.append({row[f\"image{j}\"]: preds[j] for j in range(10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc1': 0.8268772348033373,\n",
       " 'acc3': 0.9707985697258641,\n",
       " 'mrr': 0.8989807404884878}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    test_df.iloc[:, 2:-1].values,\n",
    "    test_df[\"label\"].values.reshape(-1, 1),\n",
    "    predictions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a file in <project root>/data with submissions in target format\n",
    "with open(PATH / \"blip-itm-2-400_submission.json\", 'w') as f:\n",
    "    json.dump([{k: str(v) for k, v in p.items()} for p in predictions], f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another checkpoint that might be great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1m00n/miniconda3/envs/lavis/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 3356/3356 [42:25<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc1': 0.8247914183551848, 'acc3': 0.9687127532777116, 'mrr': 0.8965728285752124}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = Classifier(blip_model).to(DEVICE)\n",
    "checkpoint.load_state_dict(torch.load(SAVE_CHECKPOINT_PATH / f\"step-250.pt\"))\n",
    "checkpoint.eval()\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for (i, batch) in enumerate(tqdm(test_ds)):\n",
    "        preds = checkpoint(to_device(batch, DEVICE)).numpy(force=True)\n",
    "        row = test_df.iloc[i]\n",
    "        predictions.append({row[f\"image{j}\"]: preds[j] for j in range(10)})\n",
    "print(\n",
    "    evaluate(\n",
    "        test_df.iloc[:, 2:-1].values,\n",
    "        test_df[\"label\"].values.reshape(-1, 1),\n",
    "        predictions,\n",
    "    )\n",
    ")\n",
    "# creates a file in <project root>/data with submissions in target format\n",
    "with open(PATH / \"blip-itm-2-250_submission.json\", 'w') as f:\n",
    "    json.dump([{k: str(v) for k, v in p.items()} for p in predictions], f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "lavis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
