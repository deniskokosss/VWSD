{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from src.data import CustomSplitLoader, ImageSet\n",
    "from src.itm import DefaultDataset, AltNSDataset, to_device, ITMClassifier\n",
    "\n",
    "from src.utils import evaluate, mrr\n",
    "from src.validation import Validation, sum_scores, div_scores, eval_batch, metric2name\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"/home/s1m00n/research/vwsd/data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "VAL2_DATA_PATH = PATH / \"valid2.data.v1.txt\"\n",
    "VAL2_GOLD_PATH = PATH / \"valid2.gold.v1.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "TEST2_DATA_PATH = PATH / \"test2.data.v1.txt\"\n",
    "TEST2_GOLD_PATH = PATH / \"test2.gold.v1.txt\"\n",
    "NUM_PICS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_BATCH_SIZE = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "\n",
    "val2_df = pd.read_csv(VAL2_DATA_PATH, sep = '\\t', header = None)\n",
    "val2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "val2_df[\"label\"] = pd.read_csv(VAL2_GOLD_PATH, sep = \"\\t\", header = None)\n",
    "\n",
    "test2_df = pd.read_csv(TEST2_DATA_PATH, sep = '\\t', header = None)\n",
    "test2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "test2_df[\"label\"] = pd.read_csv(TEST2_GOLD_PATH, sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ITMClassifier(blip_model).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"/home/s1m00n/research/vwsd/checkpoints/BLIP-itm-28/step-1000.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_img(itm_cls: ITMClassifier, data: Dict[str, torch.Tensor]):\n",
    "    enc = itm_cls.blip_model.visual_encoder.forward_features\n",
    "    proj = itm_cls.blip_model.vision_proj\n",
    "    img1_feats = F.normalize(proj(enc(data[\"img1\"])[:, 0, :]), dim=-1)\n",
    "    img2_feats = F.normalize(proj(enc(data[\"img2\"])[:, 0, :]), dim=-1)\n",
    "    return img1_feats @ img2_feats.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_proc = vis_processors[\"eval\"]\n",
    "text_proc = text_processors[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        images_path: Path,\n",
    "        vis_processor,\n",
    "    ) -> None:\n",
    "        self.images_path = images_path\n",
    "        self.vis_processor = vis_processor\n",
    "        self.data = []\n",
    "        for _, row in df.iterrows():\n",
    "            pos_pic_idx = None\n",
    "            for i in range(10):\n",
    "                if row[f\"image{i}\"] == row[\"label\"]:\n",
    "                    pos_pic_idx = i\n",
    "                    break\n",
    "            for i in range(10):\n",
    "                if i != pos_pic_idx:\n",
    "                    self.data.append({\n",
    "                        \"img1\": row[f\"image{i}\"],\n",
    "                        \"img2\": row[\"label\"],\n",
    "                    })\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        return {k: self.vis_processor(Image.open(self.images_path / v).convert(\"RGB\"))\n",
    "            for k, v in self.data[idx].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDatasetFrom10DS(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds: torch.utils.data.Dataset) -> None:\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 9 * len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        pic_idx = idx % 9\n",
    "        item_idx = int((idx - pic_idx) / 9)\n",
    "        item = self.ds[item_idx]\n",
    "        possible_indices = list(range(10))\n",
    "        img2_idx = item[\"label\"]\n",
    "        possible_indices.remove(img2_idx)\n",
    "        img1_idx = possible_indices[pic_idx]\n",
    "        return {\n",
    "            \"img1\": item[\"images\"][img1_idx],\n",
    "            \"img2\": item[\"images\"][img2_idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 16\n",
    "train_ds = ContrastiveDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "train_ds2 = ContrastiveDatasetFrom10DS(AltNSDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    "    num_negatives=9,\n",
    "    num_pics=NUM_PICS,\n",
    "))\n",
    "val_ds = ContrastiveDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val2_ds = ContrastiveDataset(\n",
    "    df=val2_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "test_ds = ContrastiveDataset(\n",
    "    df=test_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    vis_processor=vis_proc,\n",
    ")\n",
    "test2_ds = ContrastiveDataset(\n",
    "    df=test2_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    vis_processor=vis_proc,\n",
    ")\n",
    "\n",
    "dls = {\n",
    "    \"train_dl\": torch.utils.data.DataLoader(train_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"train_dl2\": torch.utils.data.DataLoader(train_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"val_dl\": torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"val2_dl\": torch.utils.data.DataLoader(val2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"test_dl\": torch.utils.data.DataLoader(test_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"test2_dl\": torch.utils.data.DataLoader(test2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = dict()\n",
    "positive_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: train_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [16:10<00:00,  7.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: train_dl2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138/138 [16:08<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: val_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [09:20<00:00,  7.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: val2_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [04:48<00:00,  8.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: test_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76/76 [09:14<00:00,  7.30s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: test2_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [04:44<00:00,  7.90s/it]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for split_name, dl in dls.items():\n",
    "    print(f\"Predicting for split: {split_name}\")\n",
    "    negative_data[split_name] = []\n",
    "    positive_data[split_name] = []\n",
    "    for batch in tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "           mixed_preds = cmp_img(model, to_device(batch, DEVICE)) \n",
    "           negative_preds = mixed_preds[:, 0]\n",
    "           positive_preds = mixed_preds[:, 1]\n",
    "           for i in range(len(negative_preds)):\n",
    "                negative_data[split_name].append(negative_preds[i].item())\n",
    "                positive_data[split_name].append(positive_preds[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH / \"itm28-pi-ni-neg-preds.json\", \"w\") as f:\n",
    "    json.dump(negative_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH / \"itm28-pi-ni-pos-preds.json\", \"w\") as f:\n",
    "    json.dump(positive_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "lavis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
