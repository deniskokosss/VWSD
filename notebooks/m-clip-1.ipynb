{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M-CLIP for VWSD\n",
    "\n",
    "WARNING: patched ~/miniconda3/envs/lavis/lib/python3.9/site-packages/multilingual_clip/pt_multilingual_clip.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multilingual_clip import pt_multilingual_clip\n",
    "import transformers\n",
    "import torch\n",
    "import clip\n",
    "import requests\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from typing import *\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from src.data import CustomSplitLoader\n",
    "from src.itm import ItmDataset, to_device, ITMClassifier\n",
    "\n",
    "from src.utils import evaluate, mrr\n",
    "from src.validation import Validation, sum_scores, div_scores, eval_batch, metric2name\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL:\n",
    "MODEL_NAME: str = \"clip-vitl14-xlmrl\" \n",
    "MODEL_VERSION: Any = 0\n",
    "DEBUG: bool = False\n",
    "\n",
    "# MODEL:\n",
    "PRETRAINED_TEXT_MODEL_NAME: str = \"M-CLIP/XLM-Roberta-Large-Vit-L-14\" \n",
    "PRETRAINED_CLIP_MODEL_NAME: str = \"ViT-L/14\"\n",
    "FROM_CHECKPOINT: Optional[str] = None # WARNING: not supported yet, \"$CHECKPOINT_PATH/$FROM_CHECKPOINT\" is loaded\n",
    "\n",
    "# TRAINING:\n",
    "NUM_EPOCHS: int = 10\n",
    "WARMUP_FRAC: float = 0.1\n",
    "GRAD_ACCUM: int = 15 # >= 1, if 1 => off\n",
    "LR: float = 1e-5\n",
    "TRAIN_BATCH_SIZE: int = 1\n",
    "TRAIN_IMG_AUG: Optional[Any] = None # augmenter\n",
    "\n",
    "# VALIDATION:\n",
    "STEPS_BETWEEN_VAL: int = 250\n",
    "STEPS_BETWEEN_CHECKPOINT: int = STEPS_BETWEEN_VAL\n",
    "VAL_BATCH_SIZE: int = 30\n",
    "\n",
    "# IMAGE NEGATIVE SAMPLING:\n",
    "NUM_SRC_PICS: int = 10 # number of pics in source table (\"image{i}\")\n",
    "NUM_NS: int = 9 # total number of negative samples for one positive\n",
    "NUM_RAND_NS: int = 0 # number of random negative samples\n",
    "NUM_HARD_NS: int = 0 # WARNING: not supported yet\n",
    "NUM_RAND_WHEN_NO_HARD_NS: int = 0 # WARNING: not supported yet\n",
    "REPLACE_DEFAULT_NS: bool = False # sampling default ns with replacement or not\n",
    "REPLACE_RAND_NS: bool = False # sampling rand ns with replacement or not\n",
    "\n",
    "# PATHS\n",
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"/home/s1m00n/research/vwsd/data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "VAL2_DATA_PATH = PATH / \"valid2.data.v1.txt\"\n",
    "VAL2_GOLD_PATH = PATH / \"valid2.gold.v1.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "TEST2_DATA_PATH = PATH / \"test2.data.v1.txt\"\n",
    "TEST2_GOLD_PATH = PATH / \"test2.gold.v1.txt\"\n",
    "CHECKPOINT_PATH = Path(\"/home/s1m00n/research/vwsd/checkpoints\").resolve() / f\"{MODEL_NAME}-{MODEL_VERSION}\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# SYSTEM:\n",
    "RANDOM_STATE = 42\n",
    "# WARNING: this is very dependent on available RAM\n",
    "NUM_WORKERS = 32\n",
    "PERSISTENT_WORKERS = True\n",
    "# WARNING: this is specific to my setup, a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "\n",
    "# AUTO DERIVED:\n",
    "TEST_BATCH_SIZE = VAL_BATCH_SIZE\n",
    "TRAIN_EFFECTIVE_BATCH_SIZE = GRAD_ACCUM * TRAIN_BATCH_SIZE\n",
    "NUM_LABELS = NUM_NS + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_STATE)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings('ignore')\n",
    "writer = SummaryWriter(f\"/home/s1m00n/research/vwsd/runs/{MODEL_NAME}-{MODEL_VERSION}\")\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_SRC_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "\n",
    "val2_df = pd.read_csv(VAL2_DATA_PATH, sep = '\\t', header = None)\n",
    "val2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "val2_df[\"label\"] = pd.read_csv(VAL2_GOLD_PATH, sep = \"\\t\", header = None)\n",
    "\n",
    "test2_df = pd.read_csv(TEST2_DATA_PATH, sep = '\\t', header = None)\n",
    "test2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "test2_df[\"label\"] = pd.read_csv(TEST2_GOLD_PATH, sep = \"\\t\", header = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCLIPClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_model,\n",
    "        clip_model,\n",
    "        tokenizer,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.text_model = text_model\n",
    "        self.clip_model = clip_model\n",
    "        self.sim = nn.CosineSimilarity(dim=1)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def forward(self, inputs: Dict[str, Any]) -> torch.Tensor:\n",
    "        text_out = self.text_model(inputs[\"text\"], self.tokenizer) # => bs, dim hidden repr\n",
    "        images = inputs[\"images\"]\n",
    "        bs = images.shape[0]\n",
    "        n = images.shape[1]\n",
    "        c = images.shape[2]\n",
    "        h = images.shape[3]\n",
    "        w = images.shape[4]\n",
    "        len_flat = bs * n\n",
    "        imgs_out = self.clip_model.encode_image(images.reshape(len_flat, c, h, w)).reshape(bs, n, -1) # => bs, n, dim hidden repr\n",
    "        if DEBUG:\n",
    "            print(\"text out:\", text_out, \"text out shape:\", text_out.shape)\n",
    "            print(\"img out:\", imgs_out, \"img out shape:\", imgs_out.shape)\n",
    "        sims_by_batch = []\n",
    "        for i in range(bs):\n",
    "            s = torch.softmax(self.sim(text_out[i].reshape(1, -1), imgs_out[i]) * 6, dim=0) \n",
    "            if DEBUG:\n",
    "                print(f\"[batch {i}] sims:\", s)\n",
    "            sims_by_batch.append(s)\n",
    "        # sims = (self.sim(text_out.reshape(1, -1), imgs_out) * 6).reshape(bs, n)\n",
    "        stack = torch.stack(sims_by_batch)\n",
    "        if DEBUG:\n",
    "            print(\"stacked batches:\", stack, \"shape:\", stack.shape)\n",
    "        return stack\n",
    "\n",
    "text_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained(PRETRAINED_TEXT_MODEL_NAME)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(PRETRAINED_TEXT_MODEL_NAME)\n",
    "clip_model, preprocess = clip.load(PRETRAINED_CLIP_MODEL_NAME)\n",
    "clip_model = clip_model.float()\n",
    "model = MCLIPClassifier(text_model, clip_model, tokenizer).to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_path: Path,\n",
    "        image_processor: Callable[[Image], torch.Tensor],\n",
    "        similarity_measure: Callable[[torch.Tensor], torch.Tensor] = nn.CosineSimilarity(dim=1),\n",
    "        enable_cache: bool = True,\n",
    "    ) -> None:\n",
    "        self.images_path = images_path\n",
    "        self.image_processor = image_processor\n",
    "        self.enable_cache = enable_cache\n",
    "        self.similarity_measure = similarity_measure\n",
    "        self.tensor_cache: Dict[str, torch.Tensor] = dict() # <file name> -> <data>\n",
    "        self.embedding_cache: Dict[str, torch.Tensor] = dict() # <file name> -> <embedding>\n",
    "        self.similarities_cache: Dict[str, Dict[str, float]] = dict() # fn1 -> fn2 -> sim(fn1, fn2)\n",
    "\n",
    "    def __getitem__(self, file_name: Union[str, List[str]]) -> torch.Tensor:\n",
    "        if isinstance(file_name, list):\n",
    "            return torch.stack([self[n] for n in file_name])\n",
    "\n",
    "        if file_name in self.tensor_cache:\n",
    "            return self.tensor_cache[file_name]\n",
    "        loaded = self.image_processor(Image.open(self.images_path / file_name))\n",
    "        if self.enable_cache:\n",
    "            self.tensor_cache[file_name] = loaded\n",
    "        return loaded\n",
    "\n",
    "    @property\n",
    "    def known_embs(self) -> List[str]:\n",
    "        return list(self.embedding_cache.keys())\n",
    "\n",
    "    def update_emb(self, file_name: str, vec: torch.Tensor):\n",
    "        self.embedding_cache[file_name] = vec\n",
    "\n",
    "    def get_emb(self, file_name: str) -> Optional[torch.Tensor]:\n",
    "        try:\n",
    "            return self.embedding_cache[file_name]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_sims(self, file_names: List[str]) -> Optional[torch.Tensor]:\n",
    "        embeddings = []\n",
    "        for name in file_names:\n",
    "            emb = self.get_emb(name)\n",
    "            if emb is None:\n",
    "                return None\n",
    "            embeddings.append(emb)\n",
    "        embeddings = torch.stack(embeddings)\n",
    "        return self.similarity_measure(embeddings)\n",
    "\n",
    "class VWSDDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        image_set: ImageSet,\n",
    "        text_preprocessor,\n",
    "        use_context_as_text: bool = True,\n",
    "        num_src_pics: int = 10,\n",
    "\n",
    "        num_ns: int = 9,\n",
    "        num_any_ns: int = 0,\n",
    "        replace_any_ns: bool = False,\n",
    "        replace_default_ns: bool = False,\n",
    "        num_hard_ns: int = 0,\n",
    "        num_any_when_no_hard_ns: int = 0,\n",
    "    ) -> None:\n",
    "        self.df = df\n",
    "        self.image_set = image_set\n",
    "        self.text_preprocessor = text_preprocessor\n",
    "        self.text_field = \"context\" if use_context_as_text else \"word\"\n",
    "        self.num_src_pics = num_src_pics\n",
    "        self.num_ns = num_ns\n",
    "        self.num_any_ns = num_any_ns\n",
    "        self.replace_any_ns = replace_any_ns\n",
    "        self.replace_default_ns = replace_default_ns\n",
    "        self.num_hard_ns = num_hard_ns\n",
    "        self.num_any_when_no_hard_ns = num_any_when_no_hard_ns\n",
    "\n",
    "        self.all_image_names: np.ndarray = np.unique(\n",
    "            self.df[[f\"image{i}\" for i in range(self.num_src_pics)]].values.ravel(\"K\")\n",
    "        )\n",
    "\n",
    "        self.num_default_ns = self.num_ns - self.num_any_ns - self.num_hard_ns\n",
    "        log.info(f\"Total pics in sample: 1 positive, {self.num_any_ns} random from all dataset, {self.num_hard_ns} hard negative samples, {self.num_default_ns} from default rows = {self.num_ns + 1} total samples\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _sample_hard_names(self, pos_img_name: str) -> Optional[List[str]]:\n",
    "        known_embs = self.image_set.known_embs\n",
    "        try:\n",
    "            pos_index = known_embs.index(pos_img_name)\n",
    "        except ValueError:\n",
    "            return None\n",
    "        else:\n",
    "            sim_mat = self.image_set.get_sims(known_embs)\n",
    "            if sim_mat is None:\n",
    "                return None\n",
    "            top_indices = torch.argsort(sim_mat[pos_index], descending=True)[:self.num_hard_ns]\n",
    "            return [known_embs[i] for i in top_indices]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict:\n",
    "        row = self.df.iloc[index]\n",
    "        pos_img_name = row[\"label\"]\n",
    "\n",
    "        negative_row_indices = []\n",
    "        for i in range(self.num_src_pics):\n",
    "            name = row[f\"image{i}\"]\n",
    "            if name != pos_img_name:\n",
    "                negative_row_indices.append(i)\n",
    "        negative_row_indices = np.array(negative_row_indices)\n",
    "\n",
    "        # making hard negatives & preparing replacements if not available\n",
    "        mb_hard_ns_names = self._sample_hard_names(pos_img_name)\n",
    "        if mb_hard_ns_names is None:\n",
    "            add_alt_ns_num = self.num_any_when_no_hard_ns\n",
    "            add_default_ns_num = self.num_hard_ns - add_alt_ns_num\n",
    "            hard_ns_names = []\n",
    "        else:\n",
    "            add_default_ns_num = 0\n",
    "            add_alt_ns_num = 0\n",
    "            hard_ns_names = mb_hard_ns_names\n",
    "        \n",
    "        # default & alt names \n",
    "        default_ns_names = [row[f\"image{i}\"] for i in np.random.choice(\n",
    "            negative_row_indices,\n",
    "            self.num_default_ns + add_default_ns_num,\n",
    "            replace = self.replace_default_ns\n",
    "        )]\n",
    "        alt_ns_names = list(np.random.choice(\n",
    "            self.all_image_names[self.all_image_names != pos_img_name],\n",
    "            self.num_any_ns + add_alt_ns_num,\n",
    "            replace=self.replace_any_ns,\n",
    "        ))\n",
    "        \n",
    "        # combine, shuffle, patch with positive\n",
    "        names = default_ns_names + alt_ns_names + hard_ns_names\n",
    "        assert len(names) == self.num_ns\n",
    "        random.shuffle(names)\n",
    "        label = random.randint(0, self.num_ns)\n",
    "        names.insert(label, pos_img_name)\n",
    "\n",
    "        return {\n",
    "            \"text\": self.text_preprocessor(row[self.text_field]),\n",
    "            \"images\": self.image_set[names], \n",
    "            \"label\": label,\n",
    "            \"image_names\": names,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# def collate(samples):\n",
    "#     texts = [s.pop(\"text\") for s in samples]\n",
    "#     collated = torch.utils.data.default_collate(samples)\n",
    "#     collated[\"text\"] = text_collator(texts)\n",
    "#     if DEBUG:\n",
    "#         for k, v in collated.items():\n",
    "#             print(k, v.shape if isinstance(v, torch.Tensor) else len(v))\n",
    "#         print(\"COLLATED TEXTS:\", collated[\"text\"])\n",
    "#     return collated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total pics in sample: 1 positive, 0 random from all dataset, 0 hard negative samples, 9 from default rows = 10 total samples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6103"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_set = ImageSet(\n",
    "    images_path = IMAGES_PATH, image_processor = preprocess,\n",
    "    similarity_measure = nn.CosineSimilarity(dim=1),\n",
    "    enable_cache = False,\n",
    ")\n",
    "train_ds = ItmDataset(\n",
    "    df = train_df,\n",
    "    image_set = train_image_set,\n",
    "    text_preprocessor = lambda x: x,\n",
    "    # text_preprocessor = tokenizer,\n",
    "    use_context_as_text = True,\n",
    "    num_src_pics = NUM_SRC_PICS, num_ns = NUM_NS, num_any_ns = NUM_RAND_NS, replace_any_ns = REPLACE_RAND_NS, replace_default_ns = REPLACE_DEFAULT_NS, num_hard_ns = NUM_HARD_NS, num_any_when_no_hard_ns = NUM_RAND_WHEN_NO_HARD_NS,\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "# train_dl = torch.utils.data.DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, collate_fn=collate)\n",
    "train_l = len(train_dl)\n",
    "train_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total pics in sample: 1 positive, 0 random from all dataset, 0 hard negative samples, 9 from default rows = 10 total samples\n",
      "INFO:root:Total pics in sample: 1 positive, 0 random from all dataset, 0 hard negative samples, 9 from default rows = 10 total samples\n"
     ]
    }
   ],
   "source": [
    "val_image_set = ImageSet(\n",
    "    images_path = IMAGES_PATH, image_processor = preprocess,\n",
    "    similarity_measure = nn.CosineSimilarity(dim=1),\n",
    "    enable_cache = False,\n",
    ")\n",
    "val_ds = ItmDataset(\n",
    "    df = validation_df,\n",
    "    image_set = val_image_set,\n",
    "    text_preprocessor = lambda x: x,\n",
    "    use_context_as_text = True,\n",
    "    num_src_pics = NUM_SRC_PICS, num_ns = NUM_NS, num_any_ns = NUM_RAND_NS, replace_any_ns = REPLACE_RAND_NS, replace_default_ns = REPLACE_DEFAULT_NS, num_hard_ns = NUM_HARD_NS, num_any_when_no_hard_ns = NUM_RAND_WHEN_NO_HARD_NS,\n",
    ")\n",
    "\n",
    "\n",
    "val2_image_set = ImageSet(\n",
    "    images_path = IMAGES_PATH, image_processor = preprocess,\n",
    "    similarity_measure = nn.CosineSimilarity(dim=1),\n",
    "    enable_cache = False,\n",
    ")\n",
    "val2_ds = ItmDataset(\n",
    "    df = val2_df,\n",
    "    image_set = val2_image_set,\n",
    "    text_preprocessor = lambda x: x,\n",
    "    # text_preprocessor = tokenizer,\n",
    "    use_context_as_text = True,\n",
    "    num_src_pics = NUM_SRC_PICS, num_ns = NUM_NS, num_any_ns = NUM_RAND_NS, replace_any_ns = REPLACE_RAND_NS, replace_default_ns = REPLACE_DEFAULT_NS, num_hard_ns = NUM_HARD_NS, num_any_when_no_hard_ns = NUM_RAND_WHEN_NO_HARD_NS,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Train settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4068 training steps which include 406 warmup ones\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = int(NUM_EPOCHS * (train_l / GRAD_ACCUM))\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_FRAC)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"{num_training_steps} training steps which include {num_warmup_steps} warmup ones\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_range = np.arange(NUM_SRC_PICS)\n",
    "if DEBUG:\n",
    "    print(\"labels range:\", labels_range)\n",
    "\n",
    "def get_batch_scores(model, batch, dev, env):\n",
    "    batch = to_device(batch, dev)\n",
    "    outputs = model(batch)\n",
    "    np_labels = batch[\"label\"].numpy(force=True)\n",
    "    np_preds = outputs.numpy(force=True)\n",
    "    if DEBUG:\n",
    "        # print(\"batch:\", batch)\n",
    "        print(\"outputs:\", outputs)\n",
    "        print(\"np labels:\", np_labels)\n",
    "        print(\"np preds:\", np_preds)\n",
    "    return {\n",
    "        \"Loss\": loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_SRC_PICS).float().to(dev)),\n",
    "        \"Accuracy@Top1\": top_k_accuracy_score(np_labels, np_preds, k=1, labels=labels_range),\n",
    "        \"Accuracy@Top3\": top_k_accuracy_score(np_labels, np_preds, k=3, labels=labels_range),\n",
    "        \"Mean Reciprocal Rank\": mrr(np_labels, np_preds),\n",
    "    }\n",
    "\n",
    "def log_score(train_step, name, metric_name, metric_value):\n",
    "    writer.add_scalar(f\"{metric_name}/{name}\", metric_value, train_step)\n",
    "    print(f\"[{train_step}][{name}]\", f\"{metric_name}: {metric_value}\")\n",
    "\n",
    "\n",
    "validation = Validation(\n",
    "    common = {\n",
    "        \"device\": DEVICE,\n",
    "        \"get_batch_scores\": get_batch_scores,\n",
    "        \"step_cond\": lambda s: (s % STEPS_BETWEEN_VAL == 0) or (s == 1),\n",
    "        \"log_score\": log_score,\n",
    "    },\n",
    "    configs = {\n",
    "        \"Validation\": { \"dl\": torch.utils.data.DataLoader(val_ds, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "        # \"Validation (augmented)\": { \"dl\": torch.utils.data.DataLoader(val_aug_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "        \"Validation 2\": { \"dl\": torch.utils.data.DataLoader(val2_ds, batch_size=VAL_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "        # \"Validation 2 (augmented)\": { \"dl\": torch.utils.data.DataLoader(val2_aug_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True), },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4068 [00:08<9:51:32,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1][Validation] Loss: 2.2632877826690674\n",
      "[1][Validation] Accuracy@Top1: 0.5709064327485384\n",
      "[1][Validation] Accuracy@Top3: 0.8036549707602337\n",
      "[1][Validation] Mean Reciprocal Rank: 0.7077473777035181\n",
      "[1][Validation 2] Loss: 2.2597692012786865\n",
      "[1][Validation 2] Accuracy@Top1: 0.6276923076923077\n",
      "[1][Validation 2] Accuracy@Top3: 0.8220512820512822\n",
      "[1][Validation 2] Mean Reciprocal Rank: 0.7459452584452584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 250/4068 [33:33<5:57:31,  5.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250][Validation] Loss: 1.845352053642273\n",
      "[250][Validation] Accuracy@Top1: 0.7786549707602337\n",
      "[250][Validation] Accuracy@Top3: 0.9573099415204682\n",
      "[250][Validation] Mean Reciprocal Rank: 0.8694843590457625\n",
      "[250][Validation 2] Loss: 1.9382201433181763\n",
      "[250][Validation 2] Accuracy@Top1: 0.6939743589743588\n",
      "[250][Validation 2] Accuracy@Top3: 0.9119230769230764\n",
      "[250][Validation 2] Mean Reciprocal Rank: 0.808325702075702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 500/4068 [1:07:10<5:34:22,  5.62s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500][Validation] Loss: 1.815487027168274\n",
      "[500][Validation] Accuracy@Top1: 0.7549707602339183\n",
      "[500][Validation] Accuracy@Top3: 0.948976608187135\n",
      "[500][Validation] Mean Reciprocal Rank: 0.8537425740276621\n",
      "[500][Validation 2] Loss: 1.9150879383087158\n",
      "[500][Validation 2] Accuracy@Top1: 0.6623076923076924\n",
      "[500][Validation 2] Accuracy@Top3: 0.8947435897435894\n",
      "[500][Validation 2] Mean Reciprocal Rank: 0.7856789275539271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 750/4068 [1:40:43<5:11:20,  5.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[750][Validation] Loss: 1.769071340560913\n",
      "[750][Validation] Accuracy@Top1: 0.7894736842105265\n",
      "[750][Validation] Accuracy@Top3: 0.9656432748538016\n",
      "[750][Validation] Mean Reciprocal Rank: 0.8784939199851476\n",
      "[750][Validation 2] Loss: 1.8763978481292725\n",
      "[750][Validation 2] Accuracy@Top1: 0.6984615384615384\n",
      "[750][Validation 2] Accuracy@Top3: 0.9182051282051277\n",
      "[750][Validation 2] Mean Reciprocal Rank: 0.8124615893365894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1000/4068 [2:14:28<4:47:31,  5.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000][Validation] Loss: 1.7807267904281616\n",
      "[1000][Validation] Accuracy@Top1: 0.7779239766081866\n",
      "[1000][Validation] Accuracy@Top3: 0.9599415204678367\n",
      "[1000][Validation] Mean Reciprocal Rank: 0.8692169080107672\n",
      "[1000][Validation 2] Loss: 1.883785367012024\n",
      "[1000][Validation 2] Accuracy@Top1: 0.6967948717948717\n",
      "[1000][Validation 2] Accuracy@Top3: 0.8998717948717947\n",
      "[1000][Validation 2] Mean Reciprocal Rank: 0.8077988909238907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 1250/4068 [2:48:01<4:23:48,  5.62s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1250][Validation] Loss: 1.7630964517593384\n",
      "[1250][Validation] Accuracy@Top1: 0.7871345029239766\n",
      "[1250][Validation] Accuracy@Top3: 0.9631578947368425\n",
      "[1250][Validation] Mean Reciprocal Rank: 0.8755490578297601\n",
      "[1250][Validation 2] Loss: 1.861405372619629\n",
      "[1250][Validation 2] Accuracy@Top1: 0.7046153846153845\n",
      "[1250][Validation 2] Accuracy@Top3: 0.9111538461538462\n",
      "[1250][Validation 2] Mean Reciprocal Rank: 0.8151474867724867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1500/4068 [3:21:29<4:00:38,  5.62s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500][Validation] Loss: 1.7696207761764526\n",
      "[1500][Validation] Accuracy@Top1: 0.7748538011695904\n",
      "[1500][Validation] Accuracy@Top3: 0.9578947368421058\n",
      "[1500][Validation] Mean Reciprocal Rank: 0.867789380859556\n",
      "[1500][Validation 2] Loss: 1.8642746210098267\n",
      "[1500][Validation 2] Accuracy@Top1: 0.7089743589743589\n",
      "[1500][Validation 2] Accuracy@Top3: 0.9189743589743589\n",
      "[1500][Validation 2] Mean Reciprocal Rank: 0.8197591066341066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 1750/4068 [3:55:07<3:37:23,  5.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1750][Validation] Loss: 1.7805321216583252\n",
      "[1750][Validation] Accuracy@Top1: 0.7666666666666668\n",
      "[1750][Validation] Accuracy@Top3: 0.9523391812865503\n",
      "[1750][Validation] Mean Reciprocal Rank: 0.8612544091710762\n",
      "[1750][Validation 2] Loss: 1.8668131828308105\n",
      "[1750][Validation 2] Accuracy@Top1: 0.6988461538461539\n",
      "[1750][Validation 2] Accuracy@Top3: 0.9194871794871792\n",
      "[1750][Validation 2] Mean Reciprocal Rank: 0.8133238705738706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2000/4068 [4:28:43<3:14:01,  5.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000][Validation] Loss: 1.7599775791168213\n",
      "[2000][Validation] Accuracy@Top1: 0.7922514619883035\n",
      "[2000][Validation] Accuracy@Top3: 0.9589181286549715\n",
      "[2000][Validation] Mean Reciprocal Rank: 0.8778649169219344\n",
      "[2000][Validation 2] Loss: 1.8498072624206543\n",
      "[2000][Validation 2] Accuracy@Top1: 0.7203846153846153\n",
      "[2000][Validation 2] Accuracy@Top3: 0.9221794871794871\n",
      "[2000][Validation 2] Mean Reciprocal Rank: 0.8266691595441594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2250/4068 [5:02:35<2:50:28,  5.63s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2250][Validation] Loss: 1.7746481895446777\n",
      "[2250][Validation] Accuracy@Top1: 0.7728070175438597\n",
      "[2250][Validation] Accuracy@Top3: 0.9516081871345038\n",
      "[2250][Validation] Mean Reciprocal Rank: 0.8650011603081781\n",
      "[2250][Validation 2] Loss: 1.859097957611084\n",
      "[2250][Validation 2] Accuracy@Top1: 0.7057692307692307\n",
      "[2250][Validation 2] Accuracy@Top3: 0.9146153846153845\n",
      "[2250][Validation 2] Mean Reciprocal Rank: 0.8169820919820917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 2500/4068 [5:36:12<2:26:18,  5.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500][Validation] Loss: 1.7911733388900757\n",
      "[2500][Validation] Accuracy@Top1: 0.7492690058479526\n",
      "[2500][Validation] Accuracy@Top3: 0.9409356725146203\n",
      "[2500][Validation] Mean Reciprocal Rank: 0.8488746170983014\n",
      "[2500][Validation 2] Loss: 1.8671339750289917\n",
      "[2500][Validation 2] Accuracy@Top1: 0.6911538461538461\n",
      "[2500][Validation 2] Accuracy@Top3: 0.9107692307692302\n",
      "[2500][Validation 2] Mean Reciprocal Rank: 0.8094856532356531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 2750/4068 [6:09:43<2:03:15,  5.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2750][Validation] Loss: 1.7849836349487305\n",
      "[2750][Validation] Accuracy@Top1: 0.7567251461988301\n",
      "[2750][Validation] Accuracy@Top3: 0.9442982456140355\n",
      "[2750][Validation] Mean Reciprocal Rank: 0.8538307574491788\n",
      "[2750][Validation 2] Loss: 1.8607451915740967\n",
      "[2750][Validation 2] Accuracy@Top1: 0.6993589743589746\n",
      "[2750][Validation 2] Accuracy@Top3: 0.9123076923076923\n",
      "[2750][Validation 2] Mean Reciprocal Rank: 0.8139547212047213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 3000/4068 [6:43:21<1:39:47,  5.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000][Validation] Loss: 1.8049863576889038\n",
      "[3000][Validation] Accuracy@Top1: 0.7299707602339186\n",
      "[3000][Validation] Accuracy@Top3: 0.9318713450292401\n",
      "[3000][Validation] Mean Reciprocal Rank: 0.8357177666388188\n",
      "[3000][Validation 2] Loss: 1.8718235492706299\n",
      "[3000][Validation 2] Accuracy@Top1: 0.6889743589743593\n",
      "[3000][Validation 2] Accuracy@Top3: 0.911153846153846\n",
      "[3000][Validation 2] Mean Reciprocal Rank: 0.8069339641839641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 3250/4068 [7:16:51<1:16:25,  5.61s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3250][Validation] Loss: 1.794877290725708\n",
      "[3250][Validation] Accuracy@Top1: 0.7418128654970758\n",
      "[3250][Validation] Accuracy@Top3: 0.9343567251461993\n",
      "[3250][Validation] Mean Reciprocal Rank: 0.8432189269469968\n",
      "[3250][Validation 2] Loss: 1.8612120151519775\n",
      "[3250][Validation 2] Accuracy@Top1: 0.6957692307692306\n",
      "[3250][Validation 2] Accuracy@Top3: 0.9164102564102563\n",
      "[3250][Validation 2] Mean Reciprocal Rank: 0.8123857855107856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 3500/4068 [7:50:17<53:02,  5.60s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3500][Validation] Loss: 1.8153852224349976\n",
      "[3500][Validation] Accuracy@Top1: 0.7175438596491233\n",
      "[3500][Validation] Accuracy@Top3: 0.921052631578948\n",
      "[3500][Validation] Mean Reciprocal Rank: 0.8266316253596951\n",
      "[3500][Validation 2] Loss: 1.8746826648712158\n",
      "[3500][Validation 2] Accuracy@Top1: 0.6778205128205129\n",
      "[3500][Validation 2] Accuracy@Top3: 0.905\n",
      "[3500][Validation 2] Mean Reciprocal Rank: 0.7997234940984939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 3750/4068 [8:23:52<29:45,  5.61s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3750][Validation] Loss: 1.815322756767273\n",
      "[3750][Validation] Accuracy@Top1: 0.7144736842105265\n",
      "[3750][Validation] Accuracy@Top3: 0.9211988304093568\n",
      "[3750][Validation] Mean Reciprocal Rank: 0.8248077369349299\n",
      "[3750][Validation 2] Loss: 1.8746932744979858\n",
      "[3750][Validation 2] Accuracy@Top1: 0.6808974358974363\n",
      "[3750][Validation 2] Accuracy@Top3: 0.9071794871794867\n",
      "[3750][Validation 2] Mean Reciprocal Rank: 0.8011983109483111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 4000/4068 [8:57:28<06:21,  5.61s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000][Validation] Loss: 1.8159812688827515\n",
      "[4000][Validation] Accuracy@Top1: 0.7143274853801173\n",
      "[4000][Validation] Accuracy@Top3: 0.9200292397660822\n",
      "[4000][Validation] Mean Reciprocal Rank: 0.8244319131161233\n",
      "[4000][Validation 2] Loss: 1.8752110004425049\n",
      "[4000][Validation 2] Accuracy@Top1: 0.6802564102564103\n",
      "[4000][Validation 2] Accuracy@Top3: 0.9071794871794869\n",
      "[4000][Validation 2] Mean Reciprocal Rank: 0.8009365588115589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4068/4068 [9:13:51<00:00,  5.59s/it]   "
     ]
    }
   ],
   "source": [
    "step_num = 0\n",
    "grad_accum_step_cnt = 0\n",
    "save_checkpoint_step_cnt = 0\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "    for batch in train_dl:\n",
    "        batch = to_device(batch, DEVICE)\n",
    "        outputs = model(batch)\n",
    "        loss = loss_fn(outputs, F.one_hot(batch[\"label\"], NUM_LABELS).float().to(DEVICE))\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_scores = sum_scores(train_scores, eval_batch(batch[\"label\"], outputs, num_labels = NUM_LABELS))\n",
    "\n",
    "        loss.backward()\n",
    "        grad_accum_step_cnt += 1\n",
    "\n",
    "        if grad_accum_step_cnt == GRAD_ACCUM: \n",
    "            writer.add_scalar(\"Learning Rate\", lr_scheduler.get_last_lr()[0], step_num)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            writer.add_scalar(\"Loss/Train\", train_loss / GRAD_ACCUM, step_num)            \n",
    "            for k, v in div_scores(train_scores, GRAD_ACCUM).items():\n",
    "                writer.add_scalar(metric2name[k] + \"/Train\", v, step_num)\n",
    "            train_loss = 0.0\n",
    "            train_scores = {\"acc1\": 0, \"acc3\": 0, \"mrr\": 0}\n",
    "            grad_accum_step_cnt = 0\n",
    "            step_num += 1\n",
    "            save_checkpoint_step_cnt += 1\n",
    "            progress_bar.update(1)\n",
    "            validation(step_num, model)\n",
    "\n",
    "        if save_checkpoint_step_cnt == STEPS_BETWEEN_CHECKPOINT:\n",
    "            save_checkpoint_step_cnt = 0\n",
    "            p = CHECKPOINT_PATH / f\"step-{step_num}.pt\"\n",
    "            torch.save(model.state_dict(), p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lavis",
   "language": "python",
   "name": "lavis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
