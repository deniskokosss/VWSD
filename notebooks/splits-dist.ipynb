{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "from typing import *\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from lavis.models import load_model_and_preprocess, BlipBase\n",
    "from lavis.processors import load_processor\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "from src.data import CustomSplitLoader, ImageSet\n",
    "from src.itm import DefaultDataset, AltNSDataset, to_device, ITMClassifier\n",
    "\n",
    "from src.utils import evaluate, mrr\n",
    "from src.validation import Validation, sum_scores, div_scores, eval_batch, metric2name\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v1\"\n",
    "PART = \"train\"\n",
    "PATH = Path(\"/home/s1m00n/research/vwsd/data\").resolve() / f\"{PART}_{DATASET_VERSION}\"\n",
    "DATA_PATH = PATH / f\"{PART}.data.{DATASET_VERSION}.txt\"\n",
    "LABELS_PATH = PATH / f\"{PART}.gold.{DATASET_VERSION}.txt\"\n",
    "IMAGES_PATH = PATH / f\"{PART}_images_{DATASET_VERSION}\"\n",
    "TRAIN_SPLIT_PATH = PATH / \"split_train.txt\"\n",
    "VALIDATION_SPLIT_PATH = PATH / \"split_valid.txt\"\n",
    "VAL2_DATA_PATH = PATH / \"valid2.data.v1.txt\"\n",
    "VAL2_GOLD_PATH = PATH / \"valid2.gold.v1.txt\"\n",
    "TEST_SPLIT_PATH = PATH / \"split_test.txt\"\n",
    "TEST2_DATA_PATH = PATH / \"test2.data.v1.txt\"\n",
    "TEST2_GOLD_PATH = PATH / \"test2.gold.v1.txt\"\n",
    "NUM_PICS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "# some images from train might not load without the following settings or warnings would be thrown\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "# a more conventional way to do this is:\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model & training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLIP_VARIANT = \"base\" # \"base\" | \"large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_BATCH_SIZE = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep='\\t', header=None)\n",
    "df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(NUM_PICS)]\n",
    "df[\"label\"] = pd.read_csv(LABELS_PATH, sep='\\t', header=None)\n",
    "\n",
    "train_df = df.loc[pd.read_csv(TRAIN_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "validation_df = df.loc[pd.read_csv(VALIDATION_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "test_df = df.loc[pd.read_csv(TEST_SPLIT_PATH, sep='\\t', header=None).T.values[0]]\n",
    "\n",
    "val2_df = pd.read_csv(VAL2_DATA_PATH, sep = '\\t', header = None)\n",
    "val2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "val2_df[\"label\"] = pd.read_csv(VAL2_GOLD_PATH, sep = \"\\t\", header = None)\n",
    "\n",
    "test2_df = pd.read_csv(TEST2_DATA_PATH, sep = '\\t', header = None)\n",
    "test2_df.columns = [\"word\", \"context\"] + [f\"image{i}\" for i in range(10)]\n",
    "test2_df[\"label\"] = pd.read_csv(TEST2_GOLD_PATH, sep = \"\\t\", header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Missing keys []\n",
      "INFO:root:load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\n"
     ]
    }
   ],
   "source": [
    "blip_model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", BLIP_VARIANT, is_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ITMClassifier(blip_model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "itm = model.blip_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_proc = vis_processors[\"eval\"]\n",
    "text_proc = text_processors[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBinaryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        images_path: Path,\n",
    "        text_processor,\n",
    "        vis_processor,\n",
    "        use_context_as_text: bool = True,\n",
    "    ) -> None:\n",
    "        self.df = df\n",
    "        self.images_path = images_path\n",
    "        self.text_processor = text_processor\n",
    "        self.vis_processor = vis_processor\n",
    "        self.text_field = \"context\" if use_context_as_text else \"word\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 10 * len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        pic_idx = idx % 10\n",
    "        row_idx = int((idx - pic_idx) / 10)\n",
    "        row = df.iloc[row_idx]\n",
    "        img_name = row[f\"image{pic_idx}\"]\n",
    "        return {\n",
    "            \"text_input\": self.text_processor(row[self.text_field]),\n",
    "            \"image\": self.vis_processor(Image.open(self.images_path / img_name).convert(\"RGB\")),\n",
    "            \"label\": img_name == row[\"label\"],\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBinaryDatasetFrom10DS(torch.utils.data.Dataset):\n",
    "    def __init__(self, ds: torch.utils.data.Dataset) -> None:\n",
    "        self.ds = ds\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return 10 * len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        pic_idx = idx % 10\n",
    "        item_idx = int((idx - pic_idx) / 10)\n",
    "        item = self.ds[item_idx]\n",
    "        return {\n",
    "            \"text_input\": item[\"text\"],\n",
    "            \"image\": item[\"images\"][pic_idx],\n",
    "            \"label\": pic_idx == item[\"label\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = 32\n",
    "train_ds = SimpleBinaryDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "train_ds2 = SimpleBinaryDatasetFrom10DS(AltNSDataset(\n",
    "    df=train_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    "    num_negatives=9,\n",
    "    num_pics=NUM_PICS,\n",
    "))\n",
    "val_ds = SimpleBinaryDataset(\n",
    "    df=validation_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "val2_ds = SimpleBinaryDataset(\n",
    "    df=val2_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_processors[\"eval\"],\n",
    "    vis_processor=vis_processors[\"eval\"],\n",
    ")\n",
    "test_ds = SimpleBinaryDataset(\n",
    "    df=test_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_proc,\n",
    "    vis_processor=vis_proc,\n",
    ")\n",
    "test2_ds = SimpleBinaryDataset(\n",
    "    df=test2_df,\n",
    "    images_path=IMAGES_PATH,\n",
    "    text_processor=text_proc,\n",
    "    vis_processor=vis_proc,\n",
    ")\n",
    "\n",
    "dls = {\n",
    "    \"train_dl\": torch.utils.data.DataLoader(train_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"train_dl2\": torch.utils.data.DataLoader(train_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"val_dl\": torch.utils.data.DataLoader(val_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"val2_dl\": torch.utils.data.DataLoader(val2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"test_dl\": torch.utils.data.DataLoader(test_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "    \"test2_dl\": torch.utils.data.DataLoader(test2_ds, batch_size=VALIDATION_BATCH_SIZE, num_workers=NUM_WORKERS),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_data = dict()\n",
    "positive_data = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: train_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [10:38<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: train_dl2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [10:37<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: val_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [06:17<00:00,  4.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: val2_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [03:13<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: test_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [06:12<00:00,  4.66s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting for split: test2_dl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [03:13<00:00,  5.08s/it]\n"
     ]
    }
   ],
   "source": [
    "itm.eval()\n",
    "for split_name, dl in dls.items():\n",
    "    print(f\"Predicting for split: {split_name}\")\n",
    "    negative_data[split_name] = {\"pi_t\": [], \"ni_t\": []}\n",
    "    positive_data[split_name] = {\"pi_t\": [], \"ni_t\": []}\n",
    "    for batch in tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "           mixed_preds = itm(to_device(batch, DEVICE)) \n",
    "           negative_preds = mixed_preds[:, 0]\n",
    "           positive_preds = mixed_preds[:, 1]\n",
    "           for i in range(len(negative_preds)):\n",
    "                cat = \"pi_t\" if batch[\"label\"][i] == True else \"ni_t\"\n",
    "                negative_data[split_name][cat].append(negative_preds[i].item())\n",
    "                positive_data[split_name][cat].append(positive_preds[i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH / \"itm0-neg-preds.json\", \"w\") as f:\n",
    "    json.dump(negative_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH / \"itm0-pos-preds.json\", \"w\") as f:\n",
    "    json.dump(positive_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "d58cbbace6bb683ad472ebf1d2c4e8e74fe86d2924a36a4a2d835b20f7c0b19a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
